{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect Articles\n",
    "Next, we collect recent articles by scraping different sites such as EDN Network, EE Times, Electronic Design, Electronics Weekly\n",
    "\n",
    "EDN Network, https://www.edn.com/?s=Practical+design+articles+and+component+selection+guides\n",
    "\n",
    "EE Times, https://www.eetimes.com/?s=New+component+announcements%2C+technology+trends%2C+and+design+techniques\n",
    "\n",
    "Electronic Design, https://www.electronicdesign.com/search?page=1&filters=%7B%22text%22%3A%22Design%20methodologies%20and%20component%20comparisons%22%2C%22page%22%3A1%2C%22status%22%3A%5B%221%22%5D%2C%22impliedSchedules%22%3Atrue%7D&sort=score\n",
    "\n",
    "Electronics Weekly, https://www.electronicsweekly.com/?s=Product+launches+and+industry+news&type=&category=0&year=0&orderby=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import hashlib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectronicsSiteScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        self.cutoff_date = datetime(2025, 4, 1)\n",
    "        self.articles = []\n",
    "        \n",
    "        # Setup Chrome options for Selenium\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument('--headless')\n",
    "        self.chrome_options.add_argument('--no-sandbox')\n",
    "        self.chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        self.chrome_options.add_argument('--disable-gpu')\n",
    "        self.chrome_options.add_argument('--window-size=1920,1080')\n",
    "        self.chrome_options.add_argument(f'--user-agent={self.headers[\"User-Agent\"]}')\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs('./intermediate_data', exist_ok=True)\n",
    "    \n",
    "    def get_selenium_driver(self):\n",
    "        \"\"\"Initialize Selenium Chrome driver\"\"\"\n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=self.chrome_options)\n",
    "            return driver\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Chrome driver: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_page_source_selenium(self, url, wait_time=10):\n",
    "        \"\"\"Get page source using Selenium\"\"\"\n",
    "        driver = self.get_selenium_driver()\n",
    "        if not driver:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            page_source = driver.page_source\n",
    "            driver.quit()\n",
    "            return page_source\n",
    "        except Exception as e:\n",
    "            print(f\"Selenium failed for {url}: {e}\")\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "            return None\n",
    "    \n",
    "    def get_page_requests(self, url, retries=3):\n",
    "        \"\"\"Fallback to requests if Selenium fails\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                session = requests.Session()\n",
    "                session.headers.update(self.headers)\n",
    "                response = session.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                if attempt == retries - 1:\n",
    "                    print(f\"Requests failed for {url}: {e}\")\n",
    "                    return None\n",
    "                time.sleep(2)\n",
    "        return None\n",
    "    \n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Try Selenium first, fallback to requests\"\"\"\n",
    "        content = self.get_page_source_selenium(url)\n",
    "        if not content:\n",
    "            content = self.get_page_requests(url)\n",
    "        return content\n",
    "    \n",
    "    def parse_date(self, date_str):\n",
    "        \"\"\"Enhanced date parsing for various formats\"\"\"\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        # Clean the date string\n",
    "        date_str = re.sub(r'Posted on|Modified on|Published|By.*', '', date_str, flags=re.IGNORECASE)\n",
    "        date_str = date_str.strip()\n",
    "        \n",
    "        # Common date patterns\n",
    "        patterns = [\n",
    "            r'(\\d{1,2})(?:st|nd|rd|th)?\\s+(\\w+)\\s+(\\d{4})',  # 3rd June 2025\n",
    "            r'(\\w+)\\s+(\\d{1,2}),?\\s+(\\d{4})',  # June 3, 2025\n",
    "            r'(\\d{4})-(\\d{2})-(\\d{2})',  # 2025-06-03\n",
    "            r'(\\d{2})/(\\d{2})/(\\d{4})',  # 06/03/2025\n",
    "            r'(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',  # 3 June 2025\n",
    "            r'(\\w{3})\\s+(\\d{1,2}),?\\s+(\\d{4})',  # Jun 3, 2025\n",
    "        ]\n",
    "        \n",
    "        months = {\n",
    "            'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6,\n",
    "            'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12,\n",
    "            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "            'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "        }\n",
    "        \n",
    "        date_str_lower = date_str.lower()\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, date_str_lower)\n",
    "            if match:\n",
    "                try:\n",
    "                    groups = match.groups()\n",
    "                    if len(groups) == 3:\n",
    "                        if groups[0].isdigit() and groups[2].isdigit():  # Day Month Year\n",
    "                            day, month, year = int(groups[0]), groups[1], int(groups[2])\n",
    "                            if month in months:\n",
    "                                return datetime(year, months[month], day)\n",
    "                        elif groups[1].isdigit():  # Month Day Year\n",
    "                            month, day, year = groups[0], int(groups[1]), int(groups[2])\n",
    "                            if month in months:\n",
    "                                return datetime(year, months[month], day)\n",
    "                        elif '-' in date_str or '/' in date_str:\n",
    "                            if len(groups[0]) == 4:  # YYYY-MM-DD\n",
    "                                return datetime(int(groups[0]), int(groups[1]), int(groups[2]))\n",
    "                            else:  # MM/DD/YYYY\n",
    "                                return datetime(int(groups[2]), int(groups[0]), int(groups[1]))\n",
    "                except:\n",
    "                    continue\n",
    "        return None\n",
    "    \n",
    "    def scrape_eetimes(self):\n",
    "        \"\"\"Scrape EE Times using RSS and direct links\"\"\"\n",
    "        articles = []\n",
    "        \n",
    "        # Try RSS feed first\n",
    "        rss_urls = [\n",
    "            \"https://www.eetimes.com/feed/\",\n",
    "            \"https://www.eetimes.com/rss/\"\n",
    "        ]\n",
    "        \n",
    "        for rss_url in rss_urls:\n",
    "            try:\n",
    "                response = requests.get(rss_url, headers=self.headers, timeout=30)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'xml')\n",
    "                    items = soup.find_all('item')\n",
    "                    \n",
    "                    for item in items:\n",
    "                        try:\n",
    "                            title = item.find('title').text.strip()\n",
    "                            link = item.find('link').text.strip()\n",
    "                            pub_date = item.find('pubDate')\n",
    "                            \n",
    "                            if pub_date:\n",
    "                                date_str = pub_date.text\n",
    "                                # Parse RSS date format: Wed, 03 Jun 2025 10:00:00 GMT\n",
    "                                article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                                \n",
    "                                if article_date > self.cutoff_date:\n",
    "                                    articles.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link,\n",
    "                                        'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                        'source': 'EE Times'\n",
    "                                    })\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # If RSS doesn't work, try direct scraping\n",
    "        if not articles:\n",
    "            content = self.get_page_content(\"https://www.eetimes.com\")\n",
    "            if content:\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                \n",
    "                # Look for article links\n",
    "                selectors = [\n",
    "                    'article .entry-title a', 'h2.entry-title a', \n",
    "                    '.post-title a', 'h3 a', '.river-block h3 a'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    links = soup.select(selector)\n",
    "                    for link in links:\n",
    "                        href = link.get('href')\n",
    "                        title = link.get_text(strip=True)\n",
    "                        \n",
    "                        if href and title and 'eetimes.com' in href:\n",
    "                            articles.append({\n",
    "                                'title': title,\n",
    "                                'url': href,\n",
    "                                'date': datetime.now().strftime('%Y-%m-%d'),  # Fallback date\n",
    "                                'source': 'EE Times'\n",
    "                            })\n",
    "                    \n",
    "                    if links:  # If we found articles with this selector, break\n",
    "                        break\n",
    "        \n",
    "        return articles[:20]  # Limit to 20 most recent\n",
    "    \n",
    "    def scrape_electronics_weekly(self):\n",
    "        \"\"\"Scrape Electronics Weekly\"\"\"\n",
    "        articles = []\n",
    "        \n",
    "        # Try RSS first\n",
    "        try:\n",
    "            response = requests.get(\"https://www.electronicsweekly.com/feed/\", \n",
    "                                  headers=self.headers, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                items = soup.find_all('item')\n",
    "                \n",
    "                for item in items:\n",
    "                    try:\n",
    "                        title = item.find('title').text.strip()\n",
    "                        link = item.find('link').text.strip()\n",
    "                        pub_date = item.find('pubDate')\n",
    "                        \n",
    "                        if pub_date:\n",
    "                            date_str = pub_date.text\n",
    "                            article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                            \n",
    "                            if article_date > self.cutoff_date:\n",
    "                                articles.append({\n",
    "                                    'title': title,\n",
    "                                    'url': link,\n",
    "                                    'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                    'source': 'Electronics Weekly'\n",
    "                                })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            # Fallback to direct scraping\n",
    "            content = self.get_page_content(\"https://www.electronicsweekly.com\")\n",
    "            if content:\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                \n",
    "                article_links = soup.select('article h2 a, .entry-title a')\n",
    "                for link in article_links:\n",
    "                    href = link.get('href')\n",
    "                    title = link.get_text(strip=True)\n",
    "                    \n",
    "                    if href and title:\n",
    "                        full_url = urljoin(\"https://www.electronicsweekly.com\", href)\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'url': full_url,\n",
    "                            'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                            'source': 'Electronics Weekly'\n",
    "                        })\n",
    "        \n",
    "        return articles[:20]\n",
    "    \n",
    "    def scrape_edn_network(self):\n",
    "        \"\"\"Scrape EDN Network\"\"\"\n",
    "        articles = []\n",
    "        \n",
    "        # Try RSS feed\n",
    "        try:\n",
    "            response = requests.get(\"https://www.edn.com/feed/\", \n",
    "                                  headers=self.headers, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                items = soup.find_all('item')\n",
    "                \n",
    "                for item in items:\n",
    "                    try:\n",
    "                        title = item.find('title').text.strip()\n",
    "                        link = item.find('link').text.strip()\n",
    "                        pub_date = item.find('pubDate')\n",
    "                        \n",
    "                        if pub_date:\n",
    "                            date_str = pub_date.text\n",
    "                            article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                            \n",
    "                            if article_date > self.cutoff_date:\n",
    "                                articles.append({\n",
    "                                    'title': title,\n",
    "                                    'url': link,\n",
    "                                    'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                    'source': 'EDN Network'\n",
    "                                })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            # Fallback to direct scraping\n",
    "            content = self.get_page_content(\"https://www.edn.com\")\n",
    "            if content:\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                \n",
    "                article_links = soup.select('.river-block h3 a, article h2 a')\n",
    "                for link in article_links:\n",
    "                    href = link.get('href')\n",
    "                    title = link.get_text(strip=True)\n",
    "                    \n",
    "                    if href and title:\n",
    "                        full_url = urljoin(\"https://www.edn.com\", href)\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'url': full_url,\n",
    "                            'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                            'source': 'EDN Network'\n",
    "                        })\n",
    "        \n",
    "        return articles[:20]\n",
    "    \n",
    "    def scrape_electronic_design(self):\n",
    "        \"\"\"Scrape Electronic Design\"\"\"\n",
    "        articles = []\n",
    "        \n",
    "        # Try RSS feed\n",
    "        try:\n",
    "            response = requests.get(\"https://www.electronicdesign.com/rss.xml\", \n",
    "                                  headers=self.headers, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                items = soup.find_all('item')\n",
    "                \n",
    "                for item in items:\n",
    "                    try:\n",
    "                        title = item.find('title').text.strip()\n",
    "                        link = item.find('link').text.strip()\n",
    "                        pub_date = item.find('pubDate')\n",
    "                        \n",
    "                        if pub_date:\n",
    "                            date_str = pub_date.text\n",
    "                            article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                            \n",
    "                            if article_date > self.cutoff_date:\n",
    "                                articles.append({\n",
    "                                    'title': title,\n",
    "                                    'url': link,\n",
    "                                    'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                    'source': 'Electronic Design'\n",
    "                                })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            # Fallback to direct scraping\n",
    "            content = self.get_page_content(\"https://www.electronicdesign.com\")\n",
    "            if content:\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                \n",
    "                article_links = soup.select('article h2 a, .post-title a')\n",
    "                for link in article_links:\n",
    "                    href = link.get('href')\n",
    "                    title = link.get_text(strip=True)\n",
    "                    \n",
    "                    if href and title:\n",
    "                        full_url = urljoin(\"https://www.electronicdesign.com\", href)\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'url': full_url,\n",
    "                            'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                            'source': 'Electronic Design'\n",
    "                        })\n",
    "        \n",
    "        return articles[:20]\n",
    "\n",
    "    def scrape_all_sites(self):\n",
    "        \"\"\"Scrape all electronics sites with improved error handling\"\"\"\n",
    "        all_articles = []\n",
    "\n",
    "        sites = [\n",
    "            (\"EE Times\", self.scrape_eetimes),\n",
    "            (\"Electronics Weekly\", self.scrape_electronics_weekly),\n",
    "            (\"EDN Network\", self.scrape_edn_network),\n",
    "            (\"Electronic Design\", self.scrape_electronic_design)\n",
    "        ]\n",
    "\n",
    "        for site_name, scrape_func in sites:\n",
    "            try:\n",
    "                articles = scrape_func()\n",
    "                all_articles.extend(articles)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape {site_name}: {e}\")\n",
    "\n",
    "            time.sleep(2)  # Rate limiting between sites\n",
    "\n",
    "        # Remove duplicates and sort\n",
    "        seen_urls = set()\n",
    "        unique_articles = []\n",
    "        for article in all_articles:\n",
    "            if article['url'] not in seen_urls:\n",
    "                unique_articles.append(article)\n",
    "                seen_urls.add(article['url'])\n",
    "\n",
    "        # Sort by date (newest first)\n",
    "        unique_articles.sort(key=lambda x: x['date'], reverse=True)\n",
    "\n",
    "        # Return the list but do NOT save here\n",
    "        return unique_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Relevant Article Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_csv(articles, path='./intermediate_data/Scraped_Article_Links.csv'):\n",
    "    if not articles:\n",
    "        print(\"No articles to save.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"\\nTotal unique articles found: {len(articles)}\")\n",
    "    print(f\"Saved to {path}\")\n",
    "\n",
    "    source_counts = df['source'].value_counts()\n",
    "    print(\"\\nArticles by source:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} articles\")\n",
    "\n",
    "scraper = ElectronicsSiteScraper()\n",
    "articles = scraper.scrape_all_sites()\n",
    "save_articles_to_csv(articles)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
