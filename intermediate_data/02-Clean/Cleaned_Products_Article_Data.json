[
  {
    "title": "EnCharge Picks The PC For Its First Analog AI Chip",
    "source": "EE Times",
    "date": "2025-06-13",
    "link": "https://www.eetimes.com/encharge-picks-the-pc-for-its-first-analog-ai-chip/",
    "text": "design Lines AI & Big Data Designline EnCharge Picks The PC For Its First Analog AI Chip By Sally Ward-Foxton 06.13.2025 0 Share Post Share on Facebook Share on Twitter Analog AI accelerator startup EnCharge AI announced its first product, the 200-TOPS (INT8) EN100 AI accelerator designed for laptops, workstations and other client devices. The device is based on the EnCharge’s capacitor-based analog compute-in-memory technology, which the company says can achieve a power efficiency above 40 TOPS/W. In-memory computing is essential for AI as it is the only technology that can provide efficient math acceleration combined with efficient data movement, EnCharge CEO Naveen Verma told EE Times. “AI is a two-sided problem,” Verma said. “On the one hand, we have a very large number of operations and so we need high efficiency for the math. On the other hand, these operations involve a lot of data and moving that data around becomes a big limitation. It turns out in-memory computing is one of the few architectures that has the potential to address both of these problems simultaneously.” Analog computing Analog computing is not a new idea, but the emergence of math-heavy AI workloads in recent years has prompted several startups to build new architectures based on some of the same concepts. In general, the basic operations of multiply and add are achieved within a memory array. A memory cell stores a weight, acting as a variable resistor with resistance in some way proportional to the weight value. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 Data is encoded onto a voltage, which when supplied to the memory cell effectively multiplies the data value with the weight value. Output wires are joined together such that currents combine as a simple form of addition. This is a very low-energy way to do multiply and add, the two math operations required for matrix multiplication, which form the bulk of AI workloads. Having computation take place in the memory—where the weights are already stored—also means less data movement is needed, which is more energy efficient. Other companies’ analog compute schemes have had various levels of success over the years. Mythic uses an array of Flash memory cells as a matrix multiply accelerator, for example, but this requires complex calibration algorithms for process and temperature variations that can reduce precision. Other types of memory can be used; Tetramem uses RRAM in its memory array. D-Matrix uses modified SRAM for analog multiply combined with digital addition in its scheme to get around problems with precision and accuracy in all-analog designs. Naveen Verma (Source: EnCharge) “While analog has the potential to give us orders of magnitude energy efficiency [advantages], the problem is we don’t build analog compute chips because it’s noisy, and it doesn’t scale,” Verma said. “The problem is semiconductor devices are variable with things like temperature and manufacturing process, so currents from these devices, which you’re trying to add up for accumulation or data reduction, these things become very noisy.” Verma’s lab at Princeton came up with a way of getting around the noise problem. Instead of using current through semiconductor devices, EnCharge instead uses charge on capacitors, generating charge and coupling capacitors together for an accumulate result. “That’s where the really critical piece comes in. These capacitors are basically just metal wires, and we have those in any foundry semiconductor technology,” Verma said, referring to the interconnect layers that are typically above the transistors in a logic or memory designs. An added benefit is that utilizing the metal layers means the compute in memory part does not take up any extra silicon area. “What’s really important is they don’t vary with temperature, they don’t have any material parameter dependencies, they are perfectly linear, they only depend on geometry,” Verma said. “And it turns out that geometry is the thing that we can control very well in CMOS.” Capacitors have very low temperature coefficients. That is, their capacitance does not vary much with temperature, Verma said. This is because, in contrast to semiconductor materials where charge flows through the material impeded by the movement of its atoms—whose effect is more pronounced at high temperatures—dielectric materials’ permeability is independent of temperature. EnCharge’s metal layer capacitors do not need the feature resolution transistors do, but CMOS’ precision helps minimize noise, Verma said. EN100 is on 16 nm, but EnCharge internal test chips have moved beyond that, he said, showing the technology is scalable with process technology. EnCharge’s EN100 on an M.2 card (Source: EnCharge) Memory cell design Other analog compute schemes have suffered from high energy penalties when converting in and out of the analog domain either side of the memory array with large numbers of DACs and ADCs. Verma said that a key differentiator of EnCharge’s capacitor-based technology is that the output is a voltage signal, not current. This means converting to voltage using transimpedance amplifiers is not needed—power and area efficient successive approximation register ADCs can be used. Transimpedance amplifiers would take three or four times the energy of the ADC itself, Verma said. “These ADCs are in the order of 20% of the area of the memory array, so they’re not big overhead in terms of area and even less in terms of energy, maybe 15 to 18%,” he said. “This has been a critical aspect in preserving the promise of analog, which in many cases gets lost when you start to build full systems.” EnCharge uses an SRAM cell of its own design, which is slightly modified from a standard foundry SRAM cell to give it the ability to control the capacitor. “There’s a lot of IP around how you build that cell in a way that remains dense, and how you co-design it with this capacitor structure in a way that preserves that fundamental intrinsic accuracy—also, in full-size, practical arrays,” Verma said. When it comes down to it, the capacitor part is the easy part—it is making the rest of the architecture as efficient as the accelerator, and developing a software stack that has kept the company busy for seven or eight years, Verma said. “Being honest, it’s great to have a spark and a big innovation like we did on the analog computing, but the real work has been in getting the architecture and the software [in place],” he said. The EN100’s analog accelerator supports 8- and 4-bit precision. For layers or operators that require higher precision, or floating point, there are digital engines on-chip. EnCharge’s compiler maps the workload across the different engines. EN100 also comes on a four-chip PCIe card (Source: EnCharge) Energy efficiency Many applications could use energy efficiency above 40 TOPS/W, but EnCharge has decided to focus on AI in the PC as a first market. “We want to be able to keep our eye on a big market opportunity where we have a very concentrated and very critical value proposition,” Verma said. “If that is your North Star, then this market is very aggressively emerging and we believe it needs us, and we want to be there to support it.” Verma said OEMs want to enable personalized or specialized models to be deployed locally on a user’s PC for compliance and security reasons. For example, 200 TOPS can enable more capable models than today’s Copilot-enabled laptops, which require 40 TOPS of acceleration. “Doing that locally means doing it under very severe power and space constraints,” Verma said. “That’s where an energy efficiency value proposition like EnCharge can bring starts to have real traction, and so that’s the path we’re following.” The difference between current AI accelerators for the PC are at around 40 TOPS, and EnCharge’s 200 TOPS marks an inflection point for client systems, Verma said. “If you’re running 1-2 billion parameter models, the models are OK, but when you get to 5, 10 or 15 billion parameters, all of a sudden they dramatically increase in capability,” he said. “Things like multimodality becomes accessible, reasoning models become accessible. The aim is for EnCharge to enable these kinds of models to become accessible on the device.” EnCharge is partnering with laptop and client platform OEMs , which is driving partnerships with ODMs. Consumer/client OEMs tend to rely on certain ODMs because the volumes they need require specialized design to manage design complexity, validation and qualification, Verma said. The company is already engaged with ISVs for testing purposes, but these engagements will move to a new phase going forward, he said. EnCharge has a range of models up and running on the EN100, including CNNs, language and vision transformers, and encoder/decoder models, largely prioritized by partner requests, Verma said. The EN100 will be available on a single-chip M.2 card with 32 GB LPDDR, with a power envelope of 8.25 W. A four-chip, half-height, half-length PCIe card offers up to 1 POPS (INT8) in a 40 W power envelope, with 128 GB LPDDR memory. Strategic customers will receive samples later this year. RELATED TOPICS: AI , AI ACCELERATOR , ANALOG , ICS , SEMICONDUCTORS Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn Sally Ward-Foxton Sally Ward-Foxton covers AI for EETimes.com and EETimes Europe magazine. Sally has spent the last 18 years writing about the electronics industry from London. She has written for Electronic Design, ECN, Electronic Specifier: Design, Components in Electronics, and many more news publications. She holds a Masters' degree in Electrical and Electronic Engineering from the University of Cambridge. Follow Sally on LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "IMS2025: Cross-correlation spectrum analyser from R&S",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/products/test-measurement-products/ims2025-cross-correlation-spectrum-analyser-from-rs-2025-06/",
    "text": "IMS2025: Cross-correlation spectrum analyser from R&S Rohde & Schwarz will demonstrate FSWX, a multi-input spectrum analyser with cross-correlation measurement at IMS2025 in an Francisco. The company emphasises a novel multi-path architecture that supports a cross-correlation mode, explaining: A single signal input is internally split into two independent signal paths, each equipped with its own local oscillator and ADC. With this design, cross-correlation algorithms can be applied in the digital back-end, effectively removing the inherent noise of the measurement instrument. This feature reveals spurs not easily seen without cross-correlation. It is especially helpful when, for instance, measuring EVM [error vector magnitude]. [Electronics Weekly has requested a diagram] The multi-path architecture also offers advanced triggering options. For example, users can apply an IF or RF power trigger at distinct frequencies, as the multi-path design allows for independent frequency settings for each receive path behind the splitter. This way, the FSWX an easily reveal effects between two RF signals. The company also emphasised the multiple inputs more than once, without saying how many. Exhaustive delving reveals the answer to be: two inputs, with models at either 26.5 or 44GHz bandwidth. “With synchronous input ports, each featuring 4GHz analysis bandwidth, users can analyze the interactions between diverse signals,” said R&S “This opens up new measurement scenarios, for instance, phase-coherent measurements of antenna arrays used in beam-forming.” There are also conventional one-input FSWX models, again with a choice of 26.5 or 44GHz bandwidth. Internal bandwidth is 8GHz, aided by avoiding YIG pre-selection filters and adopting broadband ADCs in conjunction with filter banks that “span the entire operating frequency range, allowing for pre-selected signal analysis while eliminating the need for YIG filters”, it said. “For users requiring narrowband applications, a YIG filter can still be added optionally.” Firmware is available for ‘cross application control and triggering’ (CrossACT) which synchronises measurements across different input channels allowing for simultaneous analysis with multiple tools. “This capability simplifies comparisons, such as determining whether the higher harmonics of a radar signal directly impact the error vector magnitude performance of a 5G signal,” said R&S. See FSWX spectrum analysers at the Moscone Center on stand 1,443 from 17 to 19 June, or look at this R&S web page . In April, Siglent revealed a multi-instrument mobile 5G signal analyser for 5G NR/LTE with 9kHz to 7.5GHz range and 110MHz real-time analysis bandwidth. IMS: USB and portable real-time spectrum analysers Handheld signal analyser for 5G NR Wide-band modulated load pull testing Testing Wi-Fi 7 devices for multi-path resilience Rohde and Schwarz spectrum analyser 2025-06-13 Steve Bush tweet"
  },
  {
    "title": "Dragonwing modules support varied OS",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/market-sectors/embedded-systems/dragonwing-modules-support-varied-os-2025-06/",
    "text": "Dragonwing modules support varied OS Embedded compute modules based on Qualcomm Dragonwing processors, now support different operating systems, with options including Android, Windows 11 IoT Enterprise, and Yocto Linux. Developed by Tria Technologies , an Avnet company, the boards are for embedded designs in the industrial, medical, agriculture and construction sectors, as well as edge computing, machine learning and AI. The low-power Qualcomm Dragonwing processors have edge AI and seamless networking capabilities and are scalable. The company says that the new modules offer a “powerful CPU and AI combination at lowest power consumption”. In addition, it provides Microsoft Windows with Arm, enabling a transition from the x86 architecture to Arm using the same OS. Christian Bauer, product marketing manager, Tria Technologies, commented: “This family of modules is the only one available to offer compatibility with multiple operating systems at a competitive price.” Tria has introduced four SMart Mobility Architecture COM (SMARC) standard modules and Open Standard Modules (OSM). Pictured here is the Dragonwing IQ-615 processor powered modules supporting Linux. The other options are the Dragonwing QCS5430 and QCS6490 processors, with a combination of Linux, Android and Windows 11 IOT Enterprise OS; IQ9 Series processors, supporting Linux and Snapdragon X Elite processor, supports Windows 11 IOT Enterprise OS. Related: Tria has sights on Vision AI Hardware Pioneers Video: Future-Proofing Industrial Embedded Electronics: Expert Solutions for Manufacturers IAR adopts subscription model for cloud-embedded tools Tria has sights on Vision AI with Qualcomm’s Dragonwing processor Embedded World: Profinet IRT and ProfiDrive stack for Renesas MCUs Embedded Embedded Systems Tria Technologies 2025-06-13 Caroline Hayes tweet"
  },
  {
    "title": "Tiger Lake-H Xeon-W SOSA single-board computer",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/products/bus-systems-sbcs/tiger-lake-h-xeon-w-sosa-single-board-computer-2025-06/",
    "text": "Tiger Lake-H Xeon-W SOSA single-board computer Acromag has followed IO intensive profile for its latest 3U VPX single-board computer, built around Intel’s 11th generation Tiger Lake-H Xeon W-11000E processor, and including an XMC mezzanine site. Called VPX7600, its octa-core processor supports up to 32Gbyte dual-channel soldered-down DDR4 ECC memory with data rates of up to 3.2Gtransfer/s. An Intel Gen12 UHD Xe 32EU graphics engine can clock up to 1.35GHz, and its Intel E810 Ethernet controller works with 100Gbit/s on the data plane and 10Gbit/s on the control plane. “Backplane IO includes DisplayPort, USB 3.2, SATA III, RS232/422 and GPIO,” according to the company, while “an NVMe SSD provides up to 1Tbyte of M.2 data storage” – 64Gbyte is standard. Conduction-cooled and air-cooled with front IO versions are available. Defence, aerospace and scientific applications are expected. Find the Acromag VPX7600 web page here Last month, Abaco Systems announced a SOSA-aligned Open VPX 3U video graphics and GPGPU card . 100mm single board computers get Intel Core 3 PocketBeagle 2 gets a quad core upgrade, and a GPU Nvidia RTX SOSA video graphics and GPGPU card Dev board for Nvidia Jetson Orin NX military Single Board Computers 2025-06-13 Steve Bush tweet"
  },
  {
    "title": "The 90nm Leakage Issue",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/blogs/mannerisms/yarns/the-90nm-leakage-issue-2025-06/",
    "text": "The 90nm Leakage Issue 20 years ago, as the industry transitioned from 0.13 micron to 90nm, the problem for everyone was leakage Leakage at the 90nm process node was not as big a problem for Philips Semiconductors as it was for other chip companies because Philips scaled for cost rather than performance, said its CTO Theo Claasen. “We can maintain the price erosion, but performance doesn’t scale, and power scales a little bit,” Claasen told Electronics Weekly at the 2004 MEDEA+ Forum. “The leakage problem is an Intel problem, not a Philips problem, because Intel scales for performance. We don’t need performance. The applications don’t require it. So long as we can get the density improvement and the low-power, we’re satisfied.” After the nightmare 0.13/0.12µm process node, scaling is back on track. “0.12µm was slower than expected because of yield problems,” said Claasen. “90nm is extremely smooth. If you look at 65nm, it looks pretty promising – close to the timings we expected – there are no major problems. We’ll have 65nm in production at the end of next year.” “The transistor design will be such that we reduce leakage,” added Claasen, “so we will first develop low-power versions to reduce leakage – I think we will be successful there. We shouldn’t under-estimate the influence of design – Philips has come up with current solutions which deal with all the problems.” In 2008, three years after the introduction of the 65nm process, Philips expects to have 45nm in production. Philips Semiconductors is bringing up processes to the same time-scale as the foundry, TSMC. So why does Philips bother investing in CMOS manufacturing? “To be master of our own destiny,” replied Claasen. “The cycle means you have to recover money in the up-cycle. You can make money if you have available the capacity to do that.” Intel’s Tech Predictions In 2009 The Future Of Process Developing EDA Point Tools Is Over When Arm became accepted 2025-06-13 David Manners tweet"
  },
  {
    "title": "AMS Technologies to distribute Singular Photonics’ image sensors in Europe",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/business/ams-technologies-to-distribute-singular-photonics-image-sensors-in-europe-2025-06/",
    "text": "AMS Technologies to distribute Singular Photonics’ image sensors in Europe A strategic partnership has been announced between image sensor technology specialist, Singular Photonics, and optical technology distributor AMS Technologies. The partnership means Singular’s image sensors based on single photon avalanche diodes (SPADs) will be available to European customers. AMS will be the primary distribution and support partner for European OEMs, research labs, and system integrators. Singular provides on-chip computation to SPAD-based image sensing. An example is the Andarta SPAD sensor with chip on board or CoB (pictured). Its single-photon detection technology uses real-time processing of photon-level signals to extract more information from light than conventional detection, says the company. In robotics and machine vision, this enables direct time of flight depth sensing for vision and low-light imaging for inspection and control. The SPADs are also suitable for real-time detection in bio-medical imaging and in scientific and quantum research they are used to enable time-resolved Raman spectroscopy and fluorescent lifetime imaging with low photon budgets. “The demand for single-photon sensing is growing rapidly in a wide range of sectors, and Singular Photonics is at the forefront of the latest developments in this exciting area,” said Jan Meise, CEO of AMS Technologies. “With distribution, engineering, and production under one roof, we can’t wait to help Singular accelerate their growth in key markets.” AMS Technologies will be demonstrating Singular’s sensors at Laser World of Photonics in Munich, Germany (stand B2.203). Related: Imec opens photonics lab Mouser ships BLE modules based on NXP Wi-Fi 6 chip DigiKey is upbeat for an upcycle Sponsored Content: Win Source beats the pain points in component supply for SMEs DMASS reports leap in semiconductor distribution but warns of shortages AMS Technologies Distribution photonics Singular Photonics 2025-06-13 Caroline Hayes tweet"
  },
  {
    "title": "Most Read – Qualcomm, Big Beautiful Bill, Semi sales",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/blogs/electro-ramblings/latest-news/most-read-qualcomm-big-beautiful-bill-semi-sales-2025-06/",
    "text": "Most Read – Qualcomm, Big Beautiful Bill, Semi sales For those written in the last week, our most read stories on the site cover transatlantic differences over R&D expenses, a Japanese automotive components supplier, global semiconductor sales ticking up, Builder AI, and Qualcomm buying Alphawave… Let’s take them in reverse order, as always, according to Google Analytics… 5. US ‘Big Beautiful Bill’ contrasts with UK taxing regime on R&D expenses The proposed treatment of taxation on chip R&D expenses in President Trump’s ‘Big Beautiful Bill’ contrasts with the way UK R&D expenses get taxed. The BBB proposes that US companies should be allowed to expense 100% of their domestic R&D costs from January 2025-2029 in the same year as they are incurred , rather than amortising them over five years as before. The change would provide tax relief and immediate cash flow for firms investing in R&D. Firms can deduct the full cost of R&D expenses in the year they are incurred, reducing taxable income…” 4. Marelli to file for Chapter 11 Japanese automotive components supplier Marelli Holdings, owned by US private equity outfit KKR, is expected to file for Chapter 11 in Delaware, reports the Nikkei. KKR bought Calsonic Kansei fom Nissan in 2016 for $4.5 billion and bought Magneti Marelli from Fiat Chrysler in 2019 for about $7 billion, merged the two companies under the Marelli name and loaded it up with $9.5 billion of debt which Marelli had to service by selling off bits and pieces and cost-cutting. Marelli’s debt now stands at $4.5 billion. 3. April semi sales up 22.7% y-o-y April semiconductor sales of $57 billion were up 2.5% on the March total of $55.6 billion and 22.7% up on the $46.7 billion total of April 2024, reports the SIA. “Global semiconductor sales in April ticked up on a month-to-month basis for the first time in 2025, and the global market continues to notch year-to-year growth driven by increasing sales into the Americas and Asia Pacific,” says SIA CEO John Neuffer, “meanwhile, a new WSTS industry forecast calls for solid global market growth in 2025 driven by demand for AI, cloud infrastructure, and…” 2. Builder AI goes into administration Nine year-old Builder AI, which claimed its software could make generating apps as easy as ordering a pizza, has called in the administrators with the loss of about 1,000 jobs. Builder AI raised £334 million from investors including Microsoft, Qatar’s sovereign wealth fund and Insight Partners. The company achieved a valuation of $1.5 billion. A former EY Entrepreneur of the Year, Imperial College graduate Sachin Dev Duggal founded the company and was Chief Wizard of Builder AI before leaving in February. 1. Qualcomm buys Alphawave Qualcomm has bought Alphawave for $2.4 billion in cash. The price is nearly double Alphawave’s market cap before Qualcomm was known to be interested in buying the company. Softbank and Arm also showed interest in buying Alphawave. Alphawave’s attraction is its Serdes technology. Qualcomm had previously made two all-share offers for Alphawave. Alphawave CEO Tony Pialis, said move allows Alphawave to “expand our product offerings, reach a broader customer base, and enhance our technological capabilities”. Most Read – Apple smartphones, GigaFab cluster, Qualcomm countersuit What caught your eye? (Tria Technologies, Nvidia AI, Modular satellites) Most Read – Newton Aycliffe, 12-layer HBM3E, UKRI projects Most Read articles – Digital saxophone, Qualcomm phone, Micron DRAM KKR Qualcomm 2025-06-13 Alun Williams tweet"
  },
  {
    "title": "Rugged circular connectors are latching, threaded or break-away",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/products/connectors/rugged-circular-connectors-are-latching-threaded-or-break-away-2025-06/",
    "text": "Rugged circular connectors are latching, threaded or break-away Mouser is now stocking TriMate rugged circular connectors from AirBorn, pitching them against larger legacy MIL-DTL-38999 connectors “while delivering equal or superior performance”, claimed the component distributor. These come in three mating-retention forms on the plug side: traditional 38999-style ratcheting tri-start threaded, push-pull latch, and break-away for quick de-mating. “Each plug type mates with the same receptacle to reduce manufacturing complexity and simplify the bill of materials,” said Mouser. Gold plated MIL-C-39029 contacts are used, with solid-body pin and socket construction, with anti-over-stress socket hoods. “TriMate connectors are easy to over-mould, and accept standard MIL-DTL-38999 backshells and banding accessories,” said Mouser. “The connectors are waterproof to MIL-STD-810 and withstand temperatures from -65 to +175°C.” Aerospace and defence applications are envisages, including in radar, ground vehicles, drones and robotic systems, as is use in factory automation and mining. Find Mouser’s TriMate connector web age here Glenair is another company with a different spin on 38999 connectors High power connectors for aircraft Tough and sealed power connectors for electric vehicles Sponsored Content: Automotive Waterproof Connectors Engineered for Harsh Environments Sponsored Content: Driving the Future: Evolution of Automotive Connectors Aerospace Connector industrial military 2025-06-13 Steve Bush tweet"
  },
  {
    "title": "Green lights for SES acquisition of Intelsat",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/business/green-lights-for-ses-acquition-of-intelsat-2025-06/",
    "text": "Green lights for SES acquisition of Intelsat Following Intelsat shareholder approval last week, and the UK competition authority’s recent green light, the European Commission has now given final approval for the SES acquisition of Intelsat. SES SES is the Luxembourg-based multi-orbit satellite comms giant, providing video and data connectivity to broadcasters and ISPs. It has 70 satellites in operation in MEO (Medium Earth Orbit) and GEO (Geostationary Orbit). The European Commission considered the effect on competition for the satcoms specialists to merge. Just like the UK’s Competition & Market’s Authority a few days earlier. Competition from Starlink and Viasat Inmarsat was noted, however. As was the imminent arrival of services from Amazons Kuiper. The regulator writes: “Based on its market investigation, the Commission found that there are credible competitors on those markets that, following the transaction, will continue to exert sufficient competitive pressure upon the merged entity. It also found that the merged entity will be constrained by terrestrial alternatives such as fibre in the market for the supply of ‘one-way’ satellite capacity, and by LEO operators in the market for the supply of ‘two-way’ satellite capacity.” “In addition, based on the results of the market investigation, the Commission considers that the merged entity would not have the ability to foreclose downstream competitors by restricting access to its satellite capacity.” CMA The CMA had previously assessed the potential competitive effect of the merger. But it decided not to take matters further. “The CMA found that the Merged Entity would face significant competitive pressure in the supply of broadband IFC services to commercial airlines, including from vertically integrated companies such as Starlink and Viasat Inmarsat. These competitive pressures are expected to increase further due to the planned entry of Amazon Kuiper.” “The CMA therefore found that the Merger does not give rise to a realistic prospect of an SLC as a result of input foreclosure in the supply of broadband IFC services to commercial airlines globally and in Europe (including the UK).” Talks on the merger first began in March 2023, but stalled then resumed and then faced regulatory approval. SES headquarters are at Château de Betzdorf. This was a donation to the company in the 1980’s by The Grand Duchy of Luxembourg. Image: SES – Betzdorf HQ See also : Intelsat completes vehicle satellite connectivity test Rocket Lab buys Geost for electro-optical and infrared payloads USSF selects organisations for Space Test Experiments Platform Taiwan turns to Astranis for its first dedicated comms satellite Space Symposium: Aitech unveils IQSat an AI-enabled PicoSat platform Intelsat Satellites 2025-06-13 Alun Williams tweet"
  },
  {
    "title": "Sensorless FOC for stepper motor driver",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/products/emech-enclosures/sensorless-foc-for-stepper-motor-driver-2025-06/",
    "text": "Sensorless FOC for stepper motor driver Nanotec has introduced a driver for stepper and brush-less dc (BLDC) motors with sensor-less closed-loop and field-oriented control (FOC). Intended for steppers from NEMA 14 to NEMA 34 and BLDC motors up to NEMA 23 “its sensor-less closed-loop control provides precise management of torque and speed”, according to the company. A additional ‘safe torque off’ (STO) function is in the pipeline. N6, as it will be known, also supports sensing via Hall sensors, quadrature incremental encoders and SSI (synchronous serial interface) encoders. Six digital and two analog inputs are included, as well as three feedback channels and a brake output for an external braking resistor. Intended for applications up to 300W, the controller-driver needs a supply between 12 and 57.6Vdc and can deliver up to 6A continuously. Peak current is wither 6A or 18A depending on version. Communication is also version-dependent, with a choice of EtherCAT, CANopen, Ethernet/IP, Modbus TCP or Modbus RTU. A micro-USB socked is included, and the unit can be programmed and parameterised using the company’s ‘Plug&Drive Studio’ software. Use is foreseen in laboratory automation, medical technology, packaging equipment, surface-mont component SMD assembly and winding systems. Founded in 1991, Nanotec has been based in near Munich in Germany since 2011. It has around 300 employees in Germany, Bulgaria, USA and China. Find the N6 on this Nanotec web page Late last year, Moon’s Industries introduced a family of stepper motors with large-bore hollow shafts . Allegro adds Cortex-M4 to 90V motor driver ICs Two current sensors and fan motor driver Good free book on DC drive system selection Power Integrations updates its BLDC driver design software Driver stepper motor 2025-06-13 Steve Bush tweet"
  },
  {
    "title": "Alice & Bob adopt CUDA and run 75x faster",
    "source": "Electronics Weekly",
    "date": "2025-06-13",
    "link": "https://www.electronicsweekly.com/news/business/alice-bob-adopt-cuda-2025-06/",
    "text": "Alice & Bob adopt CUDA and run 75x faster Alice & Bob, the French quantum computing developers who are integrating their quantum simulation libraries Dynamiqs with Nvidia’s CUDA-Q platform, have found that the combination accelerates the simulation of complex quantum dynamics by up to 75x. CUDA-Q is NVIDIA’s open-source quantum development platform for hybrid quantum-classical supercomputing. Dynamiqs is Alice & Bob’s quantum simulation libraries which enable high-speed simulation of both open and closed quantum systems by leveraging GPU-accelerated computing. This allows researchers to dramatically speed up the simulation of time-dependent quantum systems that rapidly change and evolve, such as QPUs. Dynamiqs expands the size of systems that can be practically simulated, and performs parameter sweeps across a wide range of conditions in a fraction of the time previously required. “Simulation is a critical step in the development of useful quantum processors, allowing us to understand how these complex quantum systems behave,” said Théau Peronnin, CEO of Alice & Bob. “Thanks to the integration with NVIDIA CUDA-Q, Dynamiqs can now run these simulations even faster, speeding up the development of our QPUs.” Simulations of complex systems – such as quantum processors with multiple qubits and physical components – are computationally challenging. Modeling the rapidly changing dynamics, complex interactions and vast number of possible quantum states requires significant computational resources to determine how the output of a simulation can change based on changing inputs. “With Dynamiqs, our goal was to make time-dependent quantum systems simulations faster, and from the beginning we decided to fully run them on GPUs, a new approach in the field,” said Ronan Gautier, a member of the core Dynamiqs development team and a Theoretical Physicist at Alice & Bob. “Now, with the help of the CUDA-Q team we can offer to our users an optimized and even faster interaction with NVIDIA’s hardware.” The long-lasting collaborative project with NVIDIA developed APIs that translate high-level programming instructions to low-level CUDA, providing the critical ability to interact with GPUs specifically optimized for quantum applications. Dynamiqs outperformed initial expectations with a 75x improvement in speed from early benchmarks. The integration will continue over the next months with performance expected to further increase. In addition to speed, Dynamiqs unlocks new capabilities for researchers, who can now use automatic differentiation to compute gradients of simulation outputs with respect to various input parameters. This tool is essential for paramount tasks like quantum optimal control, to guide the system toward the target state with high accuracy; parameter estimation, to infer unknown properties from data; and quantum state tomography, to reconstruct the quantum state from measurements. “CUDA-Q’s GPU acceleration opens up new possibilities for fast, scalable, and intelligent quantum system design and analysis,” said Tim Costa, senior director Quantum and CUDA-X at NVIDIA. “By combining this with its other capabilities, Dynamiqs is helping provide researchers with access to the state of the art in accelerated computing that they need for practical breakthroughs in quantum research.” Visit www.dynamiqs.org to access the documentation and the code repository on Github to explore how the library can enhance quantum simulations. Spain to put €808m into quantum technologies Alice & Bob raise €100m Series B US NIST quantum materials workshop open to participants Quantum Motion and Goldman Sachs join for complex calculations Quantum 2025-06-13 David Manners tweet"
  },
  {
    "title": "Time-to-digital conversion for space applications",
    "source": "EDN Network",
    "date": "2025-06-13",
    "link": "https://www.edn.com/time-to-digital-conversion-for-space-applications/",
    "text": "What is a TDC? A time-to-digital converter (TDC) is like a stopwatch measuring the elapsed interval between two events with picosecond precision, converting this into a digital value for post-processing. Many space applications require time-of-flight measurements to calculate distance, delay, or velocity. For example, an in-space servicing, assembly, and manufacturing (ISAM) spacecraft needs to determine precisely the relative location of debris before initiating rendezvous and retrieval operations. Similarly, space-domain awareness must understand the proximity and trajectory of other orbiting objects to assess any potential threat. A TDC receives two inputs: a start signal (or edge) to mark the beginning of the time interval to be measured, and a stop pulse. The delay between these is converted to a digital number for post-processing. Different architectures are typically used to implement the logic, e.g., counters, delay lines, or a time amplifier. Time-to-digital conversion is a technique used in space-based LiDAR systems to measure the time taken for a light pulse to travel to and from an object to calculate its distance. A LiDAR emits a laser pulse towards a target, which reflects off the latter’s surface, returning to the sensor. The TDC starts counting when the pulse is transmitted, stops when it is detected by the receiver, and using the speed of light, the distance is calculated as:   Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 where c is the speed of light and t is the time-of-flight. TDC in LiDAR For example, LiDAR is used by some Earth-Observation operators to measure altitude and surface changes over time, to, for example, monitor vegetation height, ice sheet or glacier thickness and melt, sea-ice elevation or relative sea level. Spaceborne LiDAR altimetry is capable of centimetre-level vertical precision and is illustrated in Figure 1 . Figure 1 The use of TDC in space-based LiDAR applications. Source: Rajan Bedi Similarly, a TDC is used in mass spectroscopy to measure how long it takes ions to travel from a source to a detector. This time-of-flight information is used to calculate the mass-to-charge ratio ( m/z ) of ions, and since the kinetic energy given to all ions is the same, time-of-flight is directly related to their mass as follows: where t is the time of flight, k a calibration constant, m the ion mass and z the ion charge. TDC in mass spectroscopy Space-based mass spectroscopy has many applications to identify and quantify chemical composition by measuring the mass-to-charge ratio of ions. For example, Earth Observation and space weather monitor the make-up of the Earth’s ionosphere and magnetosphere, including solar wind particles. Space science analyses the chemical structure of planetary atmospheres, lunar and asteroid surface composition, as well as soil or ice samples to detect organic molecules and potential signs of life. Time-of-flight mass spectroscopy is illustrated in Figure 2 . Figure 2 The use of TDC in time-of-flight mass spectroscopy. Source: Rajan Bedi TDC in optical communications Optical communication is increasingly being used to transmit data wirelessly between orbiting satellites, such as intersatellite links, or links from the ground to a spacecraft. High-throughput payloads are now using fibre to send data within sub-systems to overcome the bandwidth, loss, mass ,and EMI limitations of traditional copper communications. TDCs are used to detect when photons arrive, for timing-jitter analysis to prevent degradation of system performance, clock recovery, and synchronization for aligning and decoding incoming data streams, as shown in Figure 3 . Figure 3 The use of TDC and fibre-based optical communications within a payload. Source: Rajan Bedi TDC to calculate absolute time TDC is also used to calculate absolute time with the help of satellite navigation for applications such as quantum key distribution over long distances. Both the transmitter and the receiver use GNSS-disciplined oscillators to synchronize their local system clocks to a global time reference such as UTC or GPS time. A precise timestamp ( T event ) can be calculated as: where TGNSS epoch is the absolute time of the last GNSS PPS signal, e.g., 14:23:08 UTC , N is the number of clock cycles since the PPS, T clk is the clock period, and t fine is the sub-nanosecond fine time from interpolation. For example, if the TDC counts 8,700 clock cycles with a period of 1 ns and t fine = 0.217 ns, the resulting timestamp can be calculated as: The system concept based on optical communications is illustrated in Figure 4 . Figure 4 The use of TDC to calculate absolute time for quantum key distribution. Source: Rajan Bedi MAG-TDC00002-Sx TDC Magics Technologies NV has just released a rad-hard TDC for space applications: the MAG-TDC00002-Sx is shown in Figure 5 and can measure time delays with picosecond precision, converting this to a digital value for post processing. The device offers an SPI slave interface to connect to FPGAs/MCUs for configuration and read-out of the elapsed time. Figure 5 Magics’ TDC00002-Sx, Rad-Hard TDC that can measure time delays with picosecond precision. Source: Magics Technologies NV The MAG-TDC00002-Sx operates from a core voltage of 1.2 V, and its I/O can be powered from 1V8 to 3V3. The device consumes 20 mW (typical) and has a specified operating temperature from -40°C to 125°C. The MAG-TDC00002-Sx comes in a 17.9 x 10.8 mm, 28-pin, hermetic, ceramic COIC package. The architecture of the MAG-TDC00002-Sx and an application drawing are shown in Figure 6 : following power-up and initialization (lock) of the internal PLL, the TDC enters an IDLE state. When the device is configured, a pulse is generated on the TRIGGER output, and the TDC changes to a LISTEN mode. In this state, the internal 1.25-GHz counter is running and will be sampled on receipt of external start and stop signals. The values are saved to their corresponding registers, and both coarse and fine measurements can be read out via SPI to calculate time-of-flight. The MAG-TDC00002-Sx has automatic, internal self-calibration, which corrects for drifts due to process, voltage, temperature, and radiation degradation. Figure 6 The architecture and application drawing of MAG-TDC00002-Sx. Source: Magics Technologies NV As an example, a time-of-flight measurement between a single start and stop event resulted in the following MAG-TDC00002-Sx register data: START BIN DEL = 121 STOP BIN DEL0 = 28 START BIN CAL PERIOD = 110 STOP BIN CAL PERIOD0 = 110 START BIN CAL OFFSET = 8 STOP BIN CAL OFFSET = 9 START CNT VAL L = 4 START CNT VAL H = 0 STOP CNT VAL L0 = 14 STOP CNT VAL H0 = 0 The calculation of time-of-flight is: In terms of radiation hardness, the MAG-TDC00002-Sx has a specified SET/SEU tolerance of 60 MeV·cm 2 /mg and a total-dose immunity > 100 kRad (Si) / 1 kGy (Si). Radiation reports and ESCC9000 qualification are expected in Q3 of this year, and EM and EQM parts can be ordered today. The device is European and ITAR-free, which is advantageous if you have import/export concerns! MAG-TDC00002-Sx evaluation kit To prototype and de-risk the MAG-TDC00002-Sx, an evaluation kit is available comprising a base board and a TDC PCB as shown below. The latter fits on top of the former using the socket headers, and the base board connects to a PC using a USB Type-C cable as shown in Figure 7 . Figure 7 MAG-TDC00002-Sx evaluation kit with a base board and TDC PCB. Source: Magics Technologies NV The evaluation kit comes with software that communicates with the base board using SCPI commands to configure and use the MAG-TDC00002-Sx as shown in Figure 8 . Figure 8 A screenshot of MAG-TDC00002-Sx evaluation kit software using SCPI commands to configure the device. Source: Magics Technologies NV The rad-hard TDC is well-suited for manufacturers of Earth-Observation LiDAR instruments, space-science mass spectrometers, ISAM/space-domain awareness proximity detectors or high-throughput optical communications transceivers for calculating time-of-flight. The MAG-TDC00002-Sx can also be used to calculate absolute time for applications such as secure quantum key exchange via satellite. Further information about Magics’ MAG-TDC00002-Sx will be shared in a webinar to be broadcast on 22 nd May, and you can register using this link . Dr. Rajan Bedi is the CEO and founder of Spacechips, which designs and builds a range of advanced, AI-enabled, re-configurable, L to K-band, ultra high-throughput transponders, SDRs, Edge-based on-board processors and Mass-Memory Units for telecommunication, Earth-Observation, ISAM, SIGINT, navigation, 5G, internet and M2M/IoT satellites. The company also offers Space-Electronics Design-Consultancy, Avionics Testing, Technical-Marketing, Business-Intelligence and Training Services. ( www.spacechips.co.uk ). Related Content Clock architectures and their impact on system performance and reliability Oscilloscope special acquisition modes Resolve picoseconds using FPGA techniques Oscilloscope probing your satellite 0 comments on “ Time-to-digital conversion for space applications ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "The FCC Builds a Firewall Around US-Bound Electronics",
    "source": "EE Times",
    "date": "2025-06-12",
    "link": "https://www.eetimes.com/the-fcc-builds-a-firewall-around-us-bound-electronics/",
    "text": "design Lines Test & Measurement Designline The FCC Builds a Firewall Around US-Bound Electronics The FCC recently voted to ban some electronics testing labs with a decision that will shake up supply chains. By Emily Newton 06.12.2025 0 Share Post Share on Facebook Share on Twitter The Federal Communications Commission (FCC) voted to restrict the parties that can handle the electronics testing process for products entering the United States. What should electronics manufacturers and supply chain workers know about the proposal, and how should they alter their operations once it comes into force? What impact will the FCC’s decision have? The FCC directs the Public Safety and Homeland Security Bureau to publish a so-called Covered List of entities deemed to pose national security risks to the United States or its persons. As of September 2024 — the time of the most recent update — this assortment included less than a dozen brand names, although, notably, most are Chinese. This fact has caused some media coverage surrounding this proposal to characterize the FCC’s plans as a “Chinese ban” or similar. However, the reality is not as straightforward. Although the Covered List primarily features China-linked companies now, that could change along with the geopolitical situation. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 On May 22, 2025, the FCC voted unanimously to ban telecommunications certification bodies or testing labs directly or indirectly owned by an enterprise on the Covered List or exerting at least 10% control. The decision contains another aspect, enforced as part of its efforts to improve its oversight of the equipment authorization program. It would require telecommunications certification bodies and testing laboratories to report all entities holding at least a 5% direct or indirect equity or voting interest in them. Examining the accreditation and testing structure As of May 2025, the FCC recognized four accreditation bodies in the U.S. that can designate test labs operating in the country. Relatedly, the proposal’s coverage included mutual recognition arrangements between facilities in other countries. These arrangements establish procedures allowing participants to recognize and accept the conformity assessment results for regulatory purposes. Commission Meeting Room (Source: FCC Press) The FCC deems testing lab accreditations acceptable if a foreign designating authority and the Federal Communications Commission both recognize them as having a mutual recognition arrangement. At the time of its proposal, the FCC acknowledged 24 test lab accreditation bodies in 23 MRA-partnered nations. Testing facilities use specialty equipment to determine if the examined products can withstand their expected uses. For example, testers may place rugged military equipment on vibration tables to mimic the shocks it may receive in the real world. Although testing on products used within the United States may occur far beyond that country’s borders, the processes, results, and reliability affect the overall experiences of U.S. consumers interacting with them. That fact presumably pushed FCC members to vote as they did. If they believe there is a risk of labs on the Covered List doing something during the testing process that would make the devices capable of spying on the United States or performing a similar action that could threaten the nation’s security. In that case, it may have further spurred their choice. Reacting to FCC’s vote While commenting on the decision to proceed with this ban, FCC representatives said testing labs inside China examine about three-quarters of the world’s electronics. Additionally, the agency commented that its investigations showed some of these facilities potentially have deep connections to the Chinese Communist Party, including some linked to state-owned enterprises or the nation’s military. However, a representative from Washington D.C.’s Chinese Embassy commented that the FCC had overstretched the national security concept in a way that would harm Chinese companies. Despite the recent vote, this is an evolving situation. The FCC is seeking comments on a separate proposal that would expand the testing prohibition to all labs in China and other nations considered by the U.S. to be foreign adversaries. Statistics show that 98% of organizations have experienced negative ramifications from supply chain cyberattacks. The FCC’s vote suggests its authorities believe businesses on the Covered List could influence testing labs’ operations and introduce risks to the United States, and it applies even to those with little overall sway on the labs. Next steps Even so, parties relying on relevant testing facilities for products bound for the United States should strongly consider expanding their third-party network to include firms with no ties to the Covered List. The electronics testing process will stay the same, but they must find unaffected providers to handle their external examinations and get certified devices. Although that is a good starting point to satisfy the FCC, decision-makers should also perform their usual due diligence to assess whether new testing facilities have the equipment, capacity, and expertise necessary to meet current and future demands. The ban on Covered List facilities will likely cause a short-term surge of activity as affected supply chain managers scramble to form relationships with companies not affected by this recent change. It may complicate securing testing space in permitted facilities, meaning the wait could be longer than expected. If that happens, manufacturers may need to back planned release dates and communicate to customers that delays have impacted operations. Remaining adaptable to the electronics testing process This recent news highlights the importance of supply chain managers doing their best to stay flexible despite uncertainties outside their control. In addition to showing adaptability, they should stay abreast of this situation and consider submitting public comments during the stated periods for other relevant proposals made by the FCC or applicable bodies. RELATED TOPICS: CERIFICATION , ELECTRONICS , FCC , LABS Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn Emily Newton Emily Newton is a technical writer and the Editor-in-Chief of Revolutionized. She enjoys researching and writing about how technology is changing the industrial sector. 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential!",
    "source": "EE Times",
    "date": "2025-06-12",
    "link": "https://www.eetimes.com/gd32c231-series-mcu-redefining-cost-performance-unleashing-new-potential/",
    "text": "\"Partner Content\" allows today's industry thought leaders to share their unique insight and perspective with the greater ASPENCORE audience. Material published as \"Partner Content\" was created by or on behalf of ASPENCORE's partner(s) in conjunction with the ASPENCORE Studio team and may not reflect the views of the site and editors to which it is published.For more information on this program, email  design Lines MCU Designline GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 0 Share Post Share on Facebook Share on Twitter Beijing, China (June 12, 2025) — GigaDevice, a leading semiconductor company specializing in Flash memory , 32-bit microcontrollers (MCUs) , sensors , and analog products , today officially launched the value-packed GD32C231 series of entry-level microcontrollers , further expanding its Arm® Cortex®-M23 core product lineup. As the leader in China’s largest Arm® MCU market, GigaDevice positions the GD32C231 series as a “high-performance entry-level” solution designed to offer more competitive options for applications including small home appliances, BMS (Battery Management Systems), small-screen display devices, handheld consumer products, industrial auxiliary controls, and automotive aftermarket systems. With over 2 billion cumulative MCU shipments and a mature supply chain, GigaDevice’s newly launched GD32C231 series overcomes the performance limitations of traditional entry-level chips through innovative design. The series not only integrates a rich set of peripherals but also adopts an industrial-grade wide-voltage process and offers a comprehensive ecosystem. While maintaining exceptional cost-effectiveness, this affordable MCU supports more complex application scenarios, redefining value standards in the entry-level MCU market and ushering in a new era of “affordable yet high-spec” solutions. GD32C231 Series MCUs : The Ultimate Choice for Cost-Effectiveness The GD32C231 series MCUs deliver a significant upgrade in computing performance and peripheral features while maintaining excellent price competitiveness, achieving an ultra-high cost-performance balance. Built on Arm’s advanced Cortex®-M23 core architecture, the series offers up to 10% higher performance than Cortex®-M0+, with clock speeds reaching 48MHz. It supports efficient processing capabilities such as integer division, greatly enhancing software execution efficiency. In terms of memory configuration, the series features 32KB to 64KB of highly reliable embedded Flash and 12KB of low-power SRAM, with full memory areas equipped with ECC error correction. To meet the demands of diverse applications, multiple package options are available, including TSSOP20/LGA20, QFN28, LQFP32/QFN32, and LQFP48/QFN48. Thanks to its highly integrated chip design, the series effectively reduces the number of external components, providing users with a bill-of-materials (BOM) cost-optimized solution. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 GD32C231 Product Portfolio The Perfect Balance of Wide Voltage Support, Low Power, and Fast Wake-up Time The GD32C231 series delivers exceptional power flexibility and energy efficiency, supporting a wide operating voltage range from 1.8V to 5.5V and a broad temperature range from -40°C to 105°C. This makes it highly adaptable for deployment in harsh and demanding environments. Featuring multiple power management modes, the device consumes as little as 5μA in deep sleep mode and offers ultra-fast 2.6μs wake-up time – achieving an optimal balance between low power consumption and real-time performance. These capabilities make the GD32C231 ideal for battery-powered and portable applications. Reliable Operation for Safety-Critical Applications Engineered for reliability, the GD32C231 provides robust ESD protection – meeting 8kV contact discharge and 15kV air discharge standards. Full ECC error correction is applied across Flash and SRAM memory, helping to prevent data corruption. An integrated hardware CRC module further enhances data transmission integrity. These features ensure the MCU performs reliably in safety-critical environments such as industrial automation and automotive electronics. Highly Integrated Peripherals for Flexible Design The GD32C231 series integrates a comprehensive set of peripherals, significantly enhancing system integration and design flexibility: A 12-bit ADC with 13 external channels and 2 internal comparators for precise analog signal measurement. Up to 4 general-purpose 16-bit timers and 1 advanced 16-bit timer for versatile time-based operations. 2 high-speed SPI interfaces (including quad QSPI at 24Mbps), 2 I²C interfaces (supporting Fast Mode+ at 1Mbit/s), and 3 UARTs (up to 6Mbps) for robust serial communication. An integrated 3-channel DMA controller and 1 I²S interface for efficient peripheral data handling. With support for up to 45 GPIOs in a 48-pin package, the GD32C231 offers excellent expandability for complex designs. These rich peripheral resources empower the MCU to meet the demands of a wide range of applications – from consumer electronics to industrial control systems – with ease and reliability. GD32C231 block diagram Full-Stack Ecosystem Support for Efficient Development The GD32C231 series is backed by a comprehensive development ecosystem designed to accelerate product design and time-to-market. Standard software libraries and resources are readily available on GigaDevice’s official website. To support developers throughout the entire development cycle, GigaDevice provides extensive documentation, including datasheets, user manuals, hardware design guidelines, application notes, and porting references – enabling rapid onboarding for both hardware and software development. A complete SDK firmware package is also offered, featuring rich sample code and development board resources that cover everything from low-level drivers to advanced applications. The GD32 MCU family natively supports FreeRTOS, offering developers a lightweight, open-source, and high-efficiency real-time operating system. To streamline development even further, GigaDevice offers the GD32 Embedded Builder IDE – its proprietary development environment that integrates graphical configuration and intelligent code generation, reducing design complexity. The GD32 All-In-One Programmer supports essential Flash operations such as programming, erasing, reading, and option byte configuration. Meanwhile, the GD-Link debugger provides dual-mode SWD/JTAG support with plug-and-play functionality for a seamless debugging experience. GigaDevice also collaborates closely with third-party programming tool providers to offer customers a wide range of programming and debugging options. Additionally, the GD32C231 series is fully compatible with major international toolchains including Arm® Keil, IAR Embedded Workbench, and SEGGER Embedded Studio, ensuring flexibility across various development platforms. For typical use cases, GigaDevice provides robust application solutions and reference designs – helping developers shorten design cycles, simplify product validation, and accelerate the path to mass production. About GigaDevice GigaDevice Semiconductor Inc. is a global leading fabless supplier. Founded in April 2005, the company has continuously expanded its international footprint and established its global headquarters in Singapore in 2025. Today, GigaDevice operates branch offices across numerous countries and regions, providing localized support at customers’ fingertips. Committed to building a complete ecosystem with major product lines – Flash memory, MCU, sensor and analog – as the core driving force, GigaDevice can provide a wide range of solutions and services in the fields of industrial, automotive, computing, consumer electronics, IoT, mobile, networking and communications. GigaDevice has received the ISO26262:2018 automotive functional safety ASIL D certification, IEC 61508 functional safety product certification, as well as ISO9001, ISO14001, ISO45001, and Duns certifications. In a constant quest to expand our technology offering to customers, GigaDevice has also formed strategic alliances with leading foundries, assembly, and test plants to streamline supply chain management. For more details, please visit: www.GigaDevice.com *GigaDevice, GD32, and their logos are trademarks, or registered trademarks of GigaDevice Semiconductor Inc. Other names and brands are the property of their respective owners. RELATED TOPICS: Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness",
    "source": "EE Times",
    "date": "2025-06-12",
    "link": "https://www.eetimes.com/cupola360-rx2000-smart-patrol-camera-at-computex-for-real-time-spatial-awareness/",
    "text": "\"Partner Content\" allows today's industry thought leaders to share their unique insight and perspective with the greater ASPENCORE audience. Material published as \"Partner Content\" was created by or on behalf of ASPENCORE's partner(s) in conjunction with the ASPENCORE Studio team and may not reflect the views of the site and editors to which it is published.For more information on this program, email  design Lines AI & Big Data Designline Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 0 Share Post Share on Facebook Share on Twitter Under the theme “Eyes of AI,” ASPEED Technology’s subsidiary Cupola360 Inc. unveiled its next-generation Cupola360 Smart Patrol Camera RX2000 at COMPUTEX 2025, marking a significant leap forward in AI-powered safety and security innovation. Featuring  ultra-high-definition panoramic imaging, NDAA compliance, built-in cybersecurity, and edge AI computing, the RX2000 is designed as an open, extensible platform that redefines the landscape of real-time surveillance and situational awareness. Unlike traditional surveillance systems that focus on passive video capture, each Cupola360 camera serves as an “on-site digital twin,” enabling real-time environmental awareness and AI-driven decision-making. The system supports autonomous patrol, live panoramic streaming, and AI-powered analytics, offering a future-ready solution for intelligent monitoring across complex and dynamic environments. At the core of this advancement is the Cupola360⁺ Patrol System, which integrates 360-degree panoramic imaging with AI features such as fire and smoke detection, crowd analysis, behavioral recognition, and real-time patrol and inspection capabilities. In addition, the system can interoperate with conventional surveillance cameras as a “Spatial Commander,” merging panoramic and standard feeds in a picture-in-picture format. This hybrid visual experience fills blind spots, enhances situational awareness, and shifts the model from post-event playback to real-time threat prevention and operational efficiency, reducing manpower, response time, and total cost of ownership. Cupola360’s showcase also featured a range of standardized, ready-to-deploy solutions co-developed with ecosystem partners. These solutions are already being implemented in smart factories, healthcare facilities, data centers, and smart city infrastructures. Built on an open-platform architecture, they offer high compatibility, flexibility, and scalability, allowing customers to meet digital transformation demands with ease and speed. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 “With the RX2000 and our growing Cupola360 ecosystem, we’re not just enhancing visual monitoring. We’re redefining how spaces are understood and managed,” said CJ Hsieh, COO of ASPEED Technology. “Traditional IP cameras were built to record, not to be watched. They rely on fragmented grid views that don’t reflect how humans naturally see or think. RX2000 changes that. It acts as a spatial navigator, offering seamless 360° panoramic vision that simulates human perspective, integrates multiple camera feeds, and eliminates blind spots. Our goal is to empower our partners to deploy smarter, more responsive systems that address real-world safety, security, and operational challenges across industries.” From manufacturing and urban security to retail and tourism, Cupola360 continues to expand its footprint as a critical enabler of proactive, AI-driven safety and security infrastructure, bridging vision with action in the AIoT era. For more information, please visit  RELATED TOPICS: Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "Indian Automotive OEMs Stand to Benefit from an Open GMSL",
    "source": "EE Times",
    "date": "2025-06-12",
    "link": "https://www.eetimes.com/indian-automotive-oems-stand-to-benefit-from-an-open-gmsl/",
    "text": "design Lines Automotive Designline Indian Automotive OEMs Stand to Benefit from an Open GMSL By Yashasvini Razdan 06.12.2025 0 Share Post Share on Facebook Share on Twitter As reported last week , Analog Devices Inc. (ADI) opened its proprietary Gigabit Multimedia Serial Link (GMSL) video connectivity standard to the industry under a new OpenGMSL Association, a non-profit entity aimed at reshaping automotive and high-bandwidth embedded design globally. As we highlighted previously, the OpenGMSL initiative will operate under Reasonable and Non-Discriminatory (RAND) licensing terms and will include a certification roadmap for interoperability. In a media briefing, Anoop Aggarwal, head of automotive sales India at ADI, said that the Indian market stands to benefit significantly from the OpenGMSL initiative. “We are seeing adoption of surround view and safety systems not just in high-end cars, but increasingly in mid- and entry-level models—especially electric vehicles,” he said. In a set of written responses to EE Times, Balagopal Mayampurath, VP of automotive video and data solutions at ADI, revealed that specifications for both GMSL2 and GMSL3 will be released to association members. These specifications will include full PHY and protocol documentation, though simulation models, reference designs and verification environments are not currently planned. The association will adopt a tiered membership model—Promoter, Contributor and Adopter—each with different levels of access, participation and governance rights. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 (Source: ADI) “The goal is to create a road-proven, robust and multi-vendor interoperable video connectivity solution,” Mayampurath said. “Given our technology is already deployed in over a billion ICs and adopted by nearly 30 OEMs, we expect much quicker traction than rival standards.” The OpenGMSL framework is designed to address critical automotive needs, such as surround view, ADAS, infotainment and zonal architectures. Mayampurath told EE Times that mandatory compliance testing will be central to the initiative, with Granite River Labs leading the development of a standard compliance suite to validate silicon against ADI and partner implementations. The launch comes amid significant interest from global players. Companies that have committed to supporting OpenGMSL include Qualcomm Technologies, DENSO, Aptiv, OMNIVISION, GlobalFoundries, Hyundai Mobis, Rohde & Schwarz, TDK, Murata, Keysight and others across the semiconductor, testing and automotive OEM ecosystems. India’s market and design ecosystem in focus Highlighting GMSL’s advantages in the Indian automotive landscape, Aggarwal said, “GMSL enables ASIL-B safety compliance and addresses real implementation challenges Indian OEMs have faced with competing technologies. Power-over-coax and our ability to deliver up to 12 Gbps over a single cable—while meeting stringent EMI and EMC requirements—are proving key differentiators.” Aggarwal also stated that OpenGMSL aligns with India’s semiconductor localization strategy under the “Make in India” initiative. “GMSL technology is already proven and in use, but now that purchasing power is increasing, we are seeing adoption trickle down from high-end vehicles to mid-range and even smaller cars and two-wheelers. Now that it is open, we expect broader adoption,” he said. “As this expands, we are also evaluating semiconductor manufacturing options as part of our future strategy.” Understanding GMSL technology Mayampurath explained that the primary function of GMSL is to transport video data and Ethernet, up to 1 Gbps, within the vehicle. Architecture of the OpenGMSL Link: Standardised PHY, Data Link, and Protocol Layers Enforced by Mandatory Compliance Certification Program “GMSL allows for splitting, daisy chaining and sensor data aggregation. It supports ultra-high-definition display resolutions and has very low latency. This means that live images from a camera can be transported to a processor quickly enough for use in Level 2 and Level 3 autonomous driving systems. The technology is also in use in Level 4 systems, such as Robotaxis,” he elaborated. In terms of system design, GMSL chips are footprint-compatible across different speed grades. This allows OEMs to upgrade to higher-performance chips without redesigning the circuit board. GMSL is widely deployed, with over a billion chips on the road and adoption across nearly 30 car brands. However, it is not the only high-speed video and data connectivity standard available. MIPI A-PHY, an open standard developed by the MIPI Alliance, connects cameras, displays and sensors across longer distances—up to 15 meters. It offers data rates of up to 32 Gbps and is backed by major industry players, including Sony and Intel. ASA Motion Link, developed by the Automotive SerDes Alliance, is another open standard focused on low-latency video and safety-critical data transmission. While it is still relatively new, it is gaining industry traction for advanced driving systems. Certification and interoperability roadmap The compliance roadmap will include detailed interoperability guidelines between vendors—addressing a core challenge in multi-supplier automotive system design. “Whether it is ambient temperature fluctuations, EMI from seat motors or engine components, or the 10-year vehicle life cycle, we want GMSL to work without dropping a single frame,” Mayampurath said. The OpenGMSL specifications will ensure backward compatibility between GMSL2 and GMSL3, providing continuity and ease of adoption for Tier-1s and OEMs. While automotive remains the primary focus, ADI also sees interest from adjacent industries, including industrial robotics, aerospace, agriculture and security systems. “Non-automotive members are welcome to join the association and build OpenGMSL-compliant silicon,” Mayampurath said. Mayampurath confirmed that OpenGMSL supports zonal computing architectures, allowing edge-mounted cameras to connect to local zone aggregators and forward video data to high-performance computing units. This design aligns with the growing trend toward software-defined vehicles , where video infrastructure may enable over-the-air feature activation, such as unlocking Level 3 autonomy through a one-time software upgrade. “The expansion of connectivity requirements is happening as we speak,” Anoop concluded. “This is an opportunity for all OpenGMSL partners to step in and solve real customer problems through collaboration and innovation.” With ongoing discussions around encryption and ISO 21434 compliance, the association is also considering future security capabilities as part of its evolving roadmap. RELATED TOPICS: AUTOMOTIVE (EV/AV) Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn Yashasvini Razdan Yashasvini Razdan is the Senior India Correspondent for EE Times. With a bachelor’s degree in electronic engineering from Mumbai University and a post-graduate diploma in business journalism, she has served in various senior editorial roles in leading Indian electronics and business publications, where she reported on semiconductor innovations, deep tech startups and the Indian electronics ecosystem. She is passionate about combining her technical know-how and journalistic acumen to chronicle the evolution of India’s electronics ecosystem for a global audience. 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "Altum amps speed RF design with Quantic blocks",
    "source": "EDN Network",
    "date": "2025-06-12",
    "link": "https://www.edn.com/altum-amps-speed-rf-design-with-quantic-blocks/",
    "text": "Altum RF’s MMIC amplifiers are now part of Quantic’s plug-and-play X-MWblocks, enabling seamless integration into RF designs. The modular format streamlines design, evaluation, prototyping, and production for rapid RF and microwave system assembly. The initial offering includes five of Altum RF’s low-noise and driver amplifiers: ARF1200Q2, ARF1201Q2, ARF1202Q2, ARF1203Q2, and ARF1205Q2. These devices cover frequency bands from 13 GHz to 43.5 GHz, with noise figures as low as 1.6 dB. Additional Altum RF MMICs will join the X-MWblocks platform in the coming months.  Quantic X-Microwave offers a catalog of over 6000 RF components for configuring modules, assemblies, and subassemblies. Find Altum RF products here .  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 Quantic X-Microwave Altum RF 0 comments on “ Altum amps speed RF design with Quantic blocks ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "Amp elevates K-band throughput for LEO sats",
    "source": "EDN Network",
    "date": "2025-06-12",
    "link": "https://www.edn.com/amp-elevates-k-band-throughput-for-leo-sats/",
    "text": "Expanding Qorvo’s GaN-on-SiC SATCOM portfolio, the QPA1722 K-band power amplifier improves Low Earth Orbit (LEO) satellite performance. Qorvo reports the amplifier delivers three times the instantaneous bandwidth and 10% higher efficiency than comparable devices, all within a 38% smaller footprint. These enhancements enable higher data throughput and support more compact, efficient satellite payload designs. The QPA1722 operates from 17.7 GHz to 20.2 GHz, delivering up to 10 W (40 dBm) saturated and 6 W (37 dBm) linear output power. It provides more than 1 GHz of instantaneous bandwidth to support high data-rate applications, with 36% efficiency for improved power handling and thermal performance. Additional specifications include 26 dB small-signal gain, 35% power-added efficiency, and –25 dBc third-order intermodulation distortion.  Housed in a 6.0×5.0×1.64-mm surface-mount package, the QPA1722 is fully matched to 50 Ω with DC-grounded input and output ports. On-chip blocking capacitors follow the DC grounds at both ports.  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 The QPA1722 power amplifier is sampling now, with volume production planned for fall 2025. Evaluation kits are available upon request. QPA1722 product page Qorvo 0 comments on “ Amp elevates K-band throughput for LEO sats ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "Simultaneous sweep boosts multi-VNA test speed",
    "source": "EDN Network",
    "date": "2025-06-12",
    "link": "https://www.edn.com/simultaneous-sweep-boosts-multi-vna-test-speed/",
    "text": "Anritsu has added a simultaneous sweep feature to its ShockLine MS46131A 1-port vector network analyzer (VNA), which operates up to 43.5 GHz. The capability supports parallel 1-port S-parameter measurements across up to four MS46131A units. Simultaneous sweep enables coordinated triggering through an external signal, aligning the start of sweeps across multiple VNAs. Each unit can be configured independently with different start and stop frequencies, IF bandwidths, and point counts while performing synchronized sweeps.  Well-suited for multi-band, multi-configuration test environments, the MS46131A supports synchronized antenna characterization for LTE and Wi-Fi 7, sub-6 GHz and mmWave 5G (FR2 and FR3), and phased array validation. Remote operation is enabled via SCPI commands over uniquely assigned TCP port numbers, allowing full automation and integration into distributed test systems.  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 The simultaneous sweep feature is available with software version 2025.4.1 and supported on all MS46131A VNAs. MS46131A product page Anritsu 0 comments on “ Simultaneous sweep boosts multi-VNA test speed ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "Eval board eases battery motor-drive design",
    "source": "EDN Network",
    "date": "2025-06-12",
    "link": "https://www.edn.com/eval-board-eases-battery-motor-drive-design/",
    "text": "Powered by an eGaN FET, EPC’s EPC9196 is a 25-A RMS, 3-phase BLDC inverter optimized for 96-V to 150-V battery systems. The reference design targets medium-voltage motor drives, including steering in AGVs, traction in compact autonomous vehicles, and robotic joints. The EPC9196 is built around the EPC2304 , a 200-V, 3.5- mΩ (typical) eGaN FET in a thermally enhanced QFN package. This device enables high-efficiency operation with a peak phase current of 35 A and switching frequencies up to 150 kHz. GaN technology reduces switching losses and dead time, enabling smoother, quieter motor operation even at high PWM frequencies.  Featuring a wide input voltage range from 30 V to 170 V, the EPC9196 integrates gate drivers, housekeeping power, current and voltage sensing, overcurrent protection, and thermal monitoring. The reference design provides dv/dt control below 10 V/ns and supports both sensor-less and encoder-based control configurations. It is compatible with motor drive controller platforms from Microchip, ST, TI, and Renesas.  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 EPC9196 reference design boards cost $812.50 each and are available from DigiKey . The EPC2304 eGaN FET sells for $3.68 each in reels of 3,000 units. EPC9196 product page Efficient Power Conversion 0 comments on “ Eval board eases battery motor-drive design ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "MCUs enable USB-C Rev 2.4 designs",
    "source": "EDN Network",
    "date": "2025-06-12",
    "link": "https://www.edn.com/mcus-enable-usb-c-rev-2-4-designs/",
    "text": "Renesas offers one of the first microcontroller families to support USB-C Revision 2.4 with its RA2L2 group of low-power Arm Cortex-M23 MCUs. The updated USB Type-C cable and connector specification lowers voltage detection thresholds to 0.613 V for 1.5 -A sources and 1.165 V for 3.0-A sources, enhancing compatibility with newer USB-C cables and devices. Low-power operation makes the RA2L2 MCUs well-suited for portable devices such as USB data loggers, charging cases, barcode readers, and PC peripherals like gaming mice and keyboards. These entry-level MCUs consume just 87.5 µA/MHz in active mode, dropping to 250 nA in software standby. An independent UART clock enables wake-up from standby when receiving data from Wi-Fi or Bluetooth LE modules.  In addition to USB-C cable and connector detection up to 15 W (5 V/3A) and USB Full-Speed support, the MCUs offer a low-power UART, I3C, SSI, and CAN interfaces for design flexibility. The 48-MHz Cortex-M23 core is paired with up to 128 KB of code flash, 4 KB of data flash, and 16 KB of SRAM.  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 RA2L2 microcontrollers are now available. Samples and evaluation kits can be ordered from the Renesas website and authorized distributors. RA2L2 product page Renesas Electronics 0 comments on “ MCUs enable USB-C Rev 2.4 designs ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "The 2025 WWDC: From Intel, Apple’s Nearly Free, and the New Interfaces Are…More Shiny?",
    "source": "EDN Network",
    "date": "2025-06-12",
    "link": "https://www.edn.com/the-2025-wwdc-from-intel-apples-nearly-free-and-the-new-interfaces-aremore-shiny/",
    "text": "Have you ever heard the idiom “ripping off the band-aid? With thanks to Wiktionary for the definition, it means: To perform a painful or unpleasant but necessary action quickly so as to minimize the pain or fear associated with it. That’s not what Apple’s doing right now with the end stages of its Intel-to-Apple Silicon transition, which kicked off five years ago (fifteen years, ironically, after its previous announced transition to x86 CPUs , two decades ago). And although I’m (mostly) grateful for it on a personal level, I’m also annoyed by what’s seemingly the company’s latest (but definitely not the first, and probably also not the last) example of “ obsolescence by (software, in this case) design ”.   Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 Nearing the end of the line for Intel Upcoming MacOS “Tahoe” 26 , publicly unveiled at this week’s Worldwide Developers Conference (WWDC) and scheduled for a “gold” release later this year (September or October, judging from recent history ), still supports legacy Intel-based systems, but only four model/variant combos . That’s right; four (and in those few cases still absent any Apple Intelligence AI capabilities): MacBook Pro (16-inch, 2019) MacBook Pro (13-inch, 2020, Four Thunderbolt 3 ports) iMac (27-inch, 2020) Mac Pro (2019) My wife owns the first one on the list, courtesy of a Christmas present from yours truly last year. I’m typing these words on the second one. The other two are the “end of the line” models of the Intel-based iMac and Mac Pro series, both of which subsequently also switched to Apple Silicon-based varieties. Not included, long-time readers may have already noticed, is my storage capacity-constrained 2018 Mac mini ; its M2 Pro successor is already sitting on a shelf downstairs in storage, awaiting its turn in the spotlight (that said, I’ll probably cling to my 2018 model longer than I should in conjunction with OpenCore Legacy Patcher , even if only motivated by hacker curiosity and because I’m so fond of the no-longer-offered Space Gray color scheme…). But let’s go back to the second (also my) system in the previous bullet list. Did it also seem strange to you that Apple specifically referenced the model with “Four Thunderbolt 3 ports”? That’s because Apple also sold a 2020 model year variant with two Thunderbolt 3 ports. If you compare the specs of the two options , you’ll see that there’s at least some tech rationalization for the supported-or-not differentiation; the two-port model is based on a 8th-generation “Coffee Lake” Intel Core i5 8257U SoC, while my four-port model totes a 10th-generation “Ice Lake” Core i5 1038NG7. That said, they both support the same foundation x86 instruction set, right? And the integrated graphics is Intel Iris Plus in generation for both, too. So…🤷‍♂️ A one-year delay in sentencing for (some of) the Dipert family system stable aside (my wife’s iPhone XS Max also gets the axe later this year ) , the endgame verdict for Intel on Apple is now coming into clear view. Apple confirmed that “Tahoe” is x86’s last hurrah ; MacOS will be Arm-only beginning with next year’s (2026) spin. The subsequent 2028 edition will also drop Rosetta 2 dynamic software-translation support, so any x86-only coded apps will no longer run. And given these moves, along with Apple’s longstanding tradition of supporting not only the current but also prior two major O/S generations, it would make no sense for any developer to bother continuing to make and support “Universal” versions of apps (dual-coded for both x86 and Arm) once “Tahoe” drops off the support list in 2028…if they even wait that long, that is, considering that the predominant percentage of legacy Intel systems will be incapable of running a supported MacOS variant way before then. This forecast echoes what played out last time, when PowerPC was phased out in favor of x86 . The Liquid Glass interface The other key announcement at the pre-recorded 1.5-hour keynote that kicked off this week: which Apple itself condensed down to a 2 minute summary (draw your own “sizzle vs steak” conclusions per my recent comments on Microsoft and Google’s full and abridged equivalents ): involves the Liquid Glass UI revamp which, after conceptually originating with the two-years-back Vision Pro headset , now spans the broader product line. Translucency, rounded corners and expanded color vibrancy are its hallmarks; Apple even did a standalone video on it: It looks…OK, I guess. On the Vision Pro , the translucency makes total sense, because UI elements need to be not only spatially arranged with respect to each other but also in front of the real-life scene behind them (and in front of the user), reproduced for the eyes by front-facing cameras and embedded micro-OLEDs. But for phones, tablets, watches, and the like…again, it’s OK, I guess. I’m not trying to minimize the value of periodic visual-experience refreshes, mind you; it’s the same underlying motivation behind re-painting houses and the rooms inside them. It just feels not only derivative, given the Vision Pro heritage, but also reactionary, considering that Google announced its own UI refresh less than a month ago (Android 16 is apropos downloading to my Pixels as I type these words, in fact), and Samsung had unveiled its own in January ( six months later than originally promised , but I digress). The iPad (finally?) grows up There is, however, one aspect of Apple’s UI revamp that I’m very excited about, but which ironically has little (but not nothing ) to do with Liquid Glass. For many years now, iPads (particularly the high-end Pro variants) have offered substantial hardware potential , largely untapped by the platform’s unrealized software capabilities. Specifically, I’m talking about Apple’s ham-fisted Split View and Slide Over schemes for (supposedly) unlocking multitasking. Frankly, the only times I ever used either of them were accidental, and my only reaction was to (struggle to figure out how to) undo whatever UI abominations I’d unintentionally activated. Well, they’re both gone as of later this year . In their place is proper MacOS-like windowing , with menu bars, u ser customizable window locations and sizes (not to mention more simultaneously displayed windows) , and the like. Hallelujah. Reiterating a point I’ve made before (although software imperfections blunted its at-the-time reality), Apple will need to be careful to not cannibalize its computer sales by tablet sales going forward. That said, as I’ve also previously noted, if you’re going to get cannibalized, it might as well be by yourself, not to mention that tablets offer Apple more competitive isolation than do computers. Deep learning (local) model developer access Apple also this week announced that it was opening up developer access to its devices’ locally housed deep learning inference models for use by third-party applications. Near term, I’d frankly be more enthusiastic about this move if the models themselves were better . That said, given that we’re talking about “walled garden” Apple here, they’re the only game in town, so I guess something’s better than nothing. And longer term, Apple now clearly realizes it’s behind its competitors in the AI race and is revamping its management and dev teams in response (not to mention dissing its competitors in the presumed hope of slowing down the overall market until it can catch up), so circumstances will likely improve tangibly here, in fairly short order. Too much…too little…just right? By the way, I’m sure many of you have already noticed the across-the-board naming revamp of the various operating systems to a consistent “dominant model year” approach …i.e. although the new versions will likely all roll out later this year, they’ll be majority-in-use in 2026. Whatever 😜 (in all seriousness, the numerical disparity between, for example, current-gen iOS 18, MacOS 15 and WatchOS 11 likely resulted in at least some amount of consumer confusion). Broadly speaking, while I’m not trying to sound like Goldilocks with the header title of this concluding section of my 2025 WWDC coverage, I am feeling a bit of whiplash. Last year, Apple’s event was bloated with unrelenting AI hype ( therefore my title “jab” ), much of which still hasn’t achieved even a semblance of implementation reality even a year later (to developer and pundit dismay alike). This year, it felt like the pendulum swung (too far?) in the opposite direction , with excessive attention being drawn to minutia such as newly added gestures for AirPods earbuds and the Apple Watch (not that I can even remember the existing ones!), and no matter that the AI-powered real-time language translation facilities are welcome (albeit predictable). Maybe next year (and, dare I hope, in future years as well) Apple will navigate to the “ middle way ” between these extremes. Until then, I welcome your thoughts on this year’s event and associated announcements in the comments! — Brian Dipert is the Editor-in-Chief of the Edge AI and Vision Alliance, and a Senior Analyst at BDTI and Editor-in-Chief of InsideDSP, the company’s online . Related Content Microsoft Build 2025: Arm (and AI, of course) thrive The 2025 Google I/O conference: A deft AI pivot sustains the company’s relevance The 2024 WWDC: AI stands for Apple Intelligence, you see… The Apple 2023 WWDC: One more thing? Let’s wait and see Apple’s 2022 WWDC: Generational evolution and dramatic obsolescence Apple’s WWDC 2021: Software and services updates dominate Workarounds (and their tradeoffs) for integrated storage constraints 1 comment on “ The 2025 WWDC: From Intel, Apple’s Nearly Free, and the New Interfaces Are…More Shiny? ” bdipert June 12, 2025 p.s…Is it just me (along with the folks from Microsoft, apparently), or are any of you old enough to remember Windows Vista also having “Aero” UI flashbacks when you watch the “Liquid Glass” video embedded in my writeup?  Log in to Reply Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "AI Demand Drives Disaggregated Storage",
    "source": "EE Times",
    "date": "2025-06-11",
    "link": "https://www.eetimes.com/ai-demand-drives-disaggregated-storage/",
    "text": "design Lines AI & Big Data Designline AI Demand Drives Disaggregated Storage By Gary Hilson 06.11.2025 0 Share Post Share on Facebook Share on Twitter The massive, exponential growth of AI data is driving the need for disaggregated storage, and Western Digital sees both hard drives and SSDs in the mix. At Computex 2025, the company announced it was expanding its Open Composable Compatibility Lab (OCCL), added new SSD qualifications for its OpenFlex Data24 NVMe-over Fabric (NVMe-oF) storage platform, and introduced a new Ultrastar Data102 ORv3 Just a Bunch of Disks (JBOD) and an OpenFlex Data24 4100 with single-port SSDs. In a briefing with EE Times, Scott Hamilton, senior director of product management at Western Digital, said the flurry of announcements are aimed at helping customers scale up flexible storage infrastructures to meet the accelerated demands of AI, machine learning and data-heavy workloads as storage is pushed outside the confines of the server. “Disaggregated storage has ridden the coattails of software-defined storage,” he said. “The AI acceleration is providing a boost because storage is getting pushed outside the server.” Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 This is a departure from the hyper-converged infrastructure where storage, compute and memory are all inside the server, Hamilton said. AI has created more pressure to use the volume within the server to pack as much compute as possible so that resources like processors and memory that have much lower latency and sensitivity are more tightly connected while having the ability to share external storage. Hamilton said Western Digital’s Ultrastar JBOD HDD enclosures are designed to fit into an Open Compute Project (OCP) rack, which are a bit wider than other form factors to enable better cooling with easy front access for better manageability, as well as DC power that runs vertically at the back of the rack. SAS-connected hard drives have a role to play in AI driven data centers, Hamilton said, because so much data is being scraped and collected for various inference and machine learning workloads. “They’re a great candidate for that sort of bulk repository where you’re going to get all that data, and then ultimately through the AI data cycle, you’re going to start moving some of that data to more performant storage,” he said. Western Digital’s Ultrastar JBOD HDD enclosures are designed to fit into an OCP rack with easy front access for better manageability, as well as DC power that runs vertically at the back of the rack. (Source: Western Digital) The OpenFlex Data24 4100, meanwhile, features single-port SSDs and is aimed at cloud-based architectures that require redundancy. Hamilton said Western Digital is not just providing flexibility through its own products, but it is also qualifying multiple SSD vendors to help customers build out their storage infrastructure. SSDs from Kioxia, Phison, Sandisk and ScaleFlux are already qualified, with additional vendors in the qualification process, he said. Western Digital’s Colorado Springs-based Open OCCL is also a key part of its efforts to enable disaggregated storage at scale while avoiding vendor lock-in, Hamilton said. The vendor-neutral innovation hub is designed to accelerate industry-wide adoption of open, fabric-attached storage and software-defined storage solutions. The lab was introduced back in 2018 when NVMe-OF was new, Hamilton noted. OCCL 2.0 adds capabilities like comprehensive solutions architecture guidance for deploying disaggregated infrastructure, best practice frameworks for maximizing storage efficiency and benchmarking tools for evaluating SSD partner performance. SAS is difficult to scale, Hamilton noted, so Western Digital sees a role for NVMe to scale up disaggregated HDD storage with a unified ethernet fabric with different tiers of storage being the ideal scenario. “Fabric makes things a lot more scalable,” he said. In addition to the Computex announcements, Hamilton cited Western Digital’s collaboration with Ingrasys, a subsidiary of Foxconn Technology Group, to deliver a new flagship top-of-rack (TOR) switch with embedded storage as another approach it is taking to reduce the need for separate storage networks and enabling disaggregation. The joint effort will see Ingrasys manufacture the high-density TOR EBOF by leveraging Western Digital’s RapidFlex NVMe-oF bridge technology, which Hamilton said will essentially make an NVMe SSD look like an Ethernet SSD. RELATED TOPICS: AI AND BIG DATA , MEMORY Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn Gary Hilson Gary Hilson is a freelance writer and editor who has written thousands of words for print and pixel publications across North America. His areas of interest include software, enterprise and networking technology, research and education, sustainable transportation, and community news. His articles have been published by Network Computing, InformationWeek, Computing Canada, Computer Dealer News, Toronto Business Times, Strategy Magazine, and the Ottawa Citizen. Follow Gary on LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "Innatera Adds More Accelerators to Spiking Microcontroller",
    "source": "EE Times",
    "date": "2025-06-11",
    "link": "https://www.eetimes.com/innatera-adds-more-accelerators-to-spiking-microcontroller/",
    "text": "design Lines AI & Big Data Designline Innatera Adds More Accelerators to Spiking Microcontroller By Sally Ward-Foxton 06.11.2025 0 Share Post Share on Facebook Share on Twitter Dutch neuromorphic computing startup Innatera has launched the production version of its microcontroller, Pulsar, with both analog and digital spiking neural network accelerator fabrics, designed to take on processing of sensor data in endpoint devices. Versus the T1, a pre-production device announced a year ago , Pulsar has added an FFT accelerator, a power management unit that allows the device to go into different power saving or deep sleep states, and several new interfaces, including a camera parallel interface. The overall pipeline has also been streamlined, Innatera co-founder and CEO Sumeet Kumar told EE Times. “Sensor applications are notoriously power-constrained,” Kumar said. “Very often what developers need to do is trade off between application complexity, accuracy, and power dissipation.” Sumeet Kumar (Source: Innatera) Like the T1, Innatera claims Pulsar can lower latency up to 100× and lower energy consumption up to 500×, compared to deep neural networks implemented on mainstream digital hardware, without degradation of accuracy. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 Pulsar accelerator fabric Innatera calls its Pulsar design a microcontroller; there are actually multiple cores and accelerators on chip, including its spiking neural network fabric. It can replace application processors in some cases, especially for always-on sensing as it consumes less than a milliwatt (600 µW for radar-based presence detection and 400 µW for audio scene classification). Innatera’s main secret sauce is its accelerator for spiking neural networks (SNNs), which has both analog and digital fabrics. “The idea is that as an application developer, you have the option to pick whether to deploy on the analog or the digital fabric,” he said. “The key considerations are how aggressive your power constraints are…and at what timescale the features occur inside your input data.” The analog fabric is a massively parallel array of neurosynaptic cores that can be configured for different networks, taking up about a quarter of the silicon area of the Pulsar chip. This fabric allows inference in under a milliwatt and under a millisecond. However, it is best suited to applications with fast-moving signals, as analog states cannot be maintained indefinitely, even with buffering. Analog therefore suits applications like audio streaming. Slower-moving patterns, that perhaps need to be recognized in the hundreds of milliseconds timeframe, are better suited to the digital fabric—particularly if a bigger SNN is needed and the power budget is a little more relaxed, Kumar said. Innatera’s Pulsar includes analog and digital spiking neural network accelerators plus accelerators for CNNs, spike encoding and decoding, and FFT. (Source: Innatera) Both the analog and the digital fabrics accept spikes, run inference and output a precisely timed spike, he said, but the datapaths are different. The analog fabric is a current-mode engine; spikes are converted to current, which is manipulated all the way through the data path. The digital fabric does not manipulate current. Rather, data is converted to digital values and an array of synapses, each with its own weight memory, are activated by incoming spikes. When spikes arrive, packets of charge are produced, which are accumulated until a threshold is met, when an output spike is fired. While the digital fabric is synchronous (it has a clock), processing happens in an asynchronous way to allow for the required timing properties of spiking networks. “[Digital] neurons don’t need to be synchronized with one another,” Kumar said. “They’re not waiting for one another to fire, they fire discretely any time they see data which is relevant to them. Firing is independent to all other neurons in the system. So, the underlying fabric is synchronous, but the processing style on top of it is completely asynchronous.” Hardware encoders Pulsar includes hardware encoders and decoders—essential for converting data into and out of the spiking domain (custom encoders could also be implemented on the chip’s CPU if required, Kumar said). Also on-chip are hardware CNN and FFT accelerators, widely used in signal processing stages that might be used as part of a sensor data processing pipeline, and a variety of sensor interfaces. A RISC-V CPU is used for housekeeping, for implementing custom functions that are not covered by the on-chip accelerators, and any control or logic tasks that might happen before or after inference. “The main reason we put all of these compute fabrics on the chip is to provide developers with the flexibility that they need to pick and choose the right tool for their job,” Kumar said. “Real-world pipelines are often a combination of different models and they run in a different sequence, so having very efficient heterogeneous acceleration capabilities on chip allows the developer to do whatever they want with it.” Inference power and latency performance for Innatera’s Pulsar versus existing solutions (Source: Innatera) Software stack Innatera’s software stack, Talamo, has been further built out to interface directly with Pytorch; the company has built a Pytorch extension for SNNs. A Pytorch-based simulator can enable power consumption estimations from the early stages. Talamo is also compatible with TensorFlow. “Most importantly, we’ve managed to achieve ease of development,” Kumar said. “You no longer need a neuromorphic Ph.D. to be building spiking neural networks and running them on our chips—and this is actual feedback that has come from customers that have been sampling the hardware and the software through 2024.” On Talamo’s roadmap are features set to make model porting even simpler, Kumar said. Customer traction Innatera technology is already in customer deployments, with a “significant” pipeline, Kumar said. Customer applications are across presence detection, people counting, anomaly detection and classification and vital signs monitoring. Pulsar evaluation kit. (Source: Innatera) A reference design built with Socionext performs human presence detection using radar to avoid false positives because it does not use motion as a proxy for presence. This design could be used in home appliances to avoid detecting pets, while making sure to detect humans who are not moving, he said. This is useful in smart doorbells and other appliances that switch off when a person is not present. Motion detection using radar usually requires 14 mW of power for processing; Innatera’s presence detection uses just 0.6 mW on average (power draw is higher during inference). Another reference design uses a Melexis infrared sensor for people counting. The target application is a smoke detector for hotels that counts how many people are in the room in the event of a fire, without invading privacy, but it can be used in any occupancy application, Kumar said. The solution can detect humans while filtering out other heat sources like laptops or coffee cups. Other customers are using Innatera chips for audio scene classification and keyword spotting. “There are some sensor customers that will be integrating these chips into their own modules to go out into the market,” Kumar said. “All of this has been the progress since last year, so it’s a very, very promising pipeline. It’s it’s a lot more tangible than it was a few years ago.” Spiking networks Biology-inspired SNNs are still a very active field of research. Are we truly at the stage where SNNs are ready for commercial adoption? “The neuromorphic space is moving pretty quickly,” Kumar said. “In 2020, training spiking neural networks was still quite hard. In the years after that, a number of new algorithms came out, which greatly simplified training, and made it as simple as it is for conventional, non-spiking models today.” While the spiking space is still evolving quickly, Kumar feels confident that Innatera’s hardware is sufficiently programmable to cope with even novel types of spiking networks today and in the future. The fundamentals of spiking networks—dynamical systems of discrete compute elements with time-based processing, with complex relationships between neurons (like delays)—have always been present in the company’s architecture, he said. Pulsar, Innatera’s spiking microcontroller. (Source: Innatera) “These are temporal machines, which are inherently asynchronous, you have internal recurrence inside the neurons, you have the ability to create recurrent connections inside the topology, you can do [novel] topologies [outside of feedforward or recurrent networks]…you have the ability to do all that and train them, and do something meaningful,” he said. The main limitation of the Innatera fabric is that it is not self-learning, Kumar said, noting that the neuron types are fixed, chosen based on their suitability for a wide range of pattern recognition operations across a wide range of sensor types. While functions cannot be changed, parameters can, he said. This programmability is sufficient to do a large set of pattern recognition and signal processing operations very efficiently. “The key questions are really, can you represent that network well enough in Python, and can you train it with the infrastructure you’ve got?” he said. “As long as you can answer yes to both questions, then yes, you can implement your network on our chip.” The next generation of Innatera hardware will continue to add programmability, with the ultimate ambition of integrating even more closely with the sensor, Kumar said. “If you look into the brain of a locust, it has a natural ability to filter the sound of predators by implementing an FFT very efficiently using spiking neurons,” he said. “That’s really cool, because now there’s a role model for doing signal processing that we spend a lot of energy on in the electronics world with spiking neural networks.” Future Innatera chips will tap into functions like this to move into spaces like control and learning in robotics , Kumar said. Pulsar will ramp to volume production by the end of the year. RELATED TOPICS: AI , EDGE AI , ICS , MICROCONTROLLER , NEUROMORPHIC , SEMICONDUCTORS Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn Sally Ward-Foxton Sally Ward-Foxton covers AI for EETimes.com and EETimes Europe magazine. Sally has spent the last 18 years writing about the electronics industry from London. She has written for Electronic Design, ECN, Electronic Specifier: Design, Components in Electronics, and many more news publications. She holds a Masters' degree in Electrical and Electronic Engineering from the University of Cambridge. Follow Sally on LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "Can Neocloud Architectures Break the AI Infra Bottleneck?",
    "source": "EE Times",
    "date": "2025-06-11",
    "link": "https://www.eetimes.com/can-neocloud-architectures-break-the-ai-infra-bottleneck/",
    "text": "design Lines AI & Big Data Designline Can Neocloud Architectures Break the AI Infra Bottleneck? By Durgesh Srivastava 06.11.2025 0 Share Post Share on Facebook Share on Twitter I recently spoke with a startup founder who had dumped nearly $400,000 into AI infrastructure before giving up. His team had brilliant ideas but could not afford the computing power to bring them to life. This is not an isolated case. While tech giants throw billions at massive AI data centers , the rest of us face brutal barriers when trying to deploy custom AI solutions. Just renting four Nvidia H100 GPU instances for a month can cost upwards of $300,000 through traditional cloud providers. Factor in the headaches of cooling systems, power distribution, and specialized networking, and you have got a perfect storm that locks out most innovators. The traditional platform-as-a-service options are not much help either. They demand extensive configuration across a hodgepodge of hardware environments. And getting computing power physically close to your operations to minimize latency is extremely difficult. Enter neocloud: AI infrastructure’s disruptor Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 Neoclouds are a new breed of cloud solutions built specifically for AI workloads. Think of neoclouds as the food trucks of the computing world. While traditional data centers are like massive restaurants with huge overhead, neoclouds are nimble, specialized and go where they are needed. These upstart providers have reimagined how AI infrastructure gets deployed. I have tracked several neocloud providers over the past year, and the economics are compelling. When purchasing equivalent H100 instances from a neocloud versus a hyperscaler, costs typically drop by 60-70%. One CTO I spoke with slashed their compute costs from $98/hour to $34/hour simply by switching providers. What makes neoclouds truly game-changing? First, they have ruthlessly eliminated complexity. Some use prefabricated components to slash deployment time from months to days. You do not need a Ph.D. in thermal management to get started. Second, they have stripped away the virtualization layers that hobble performance in traditional clouds. This bare-metal approach means your AI workloads run at native speeds without the “noisy neighbor” problem plaguing shared infrastructure. Neoclouds eliminate complexity, with some using prefabricated components to slash deployment time from months to days; they’ve stripped away the virtualization layers that hobble performance in traditional clouds, and they have mastered scalability. (Image: DataraAI) Finally, they have mastered scalability without the red tape. One machine learning engineer told me: “We scaled from four to 40 nodes overnight when our model started showing promise. Try doing that with traditional providers.” Physical AI: where the digital and physical worlds collide The real magic happens when you pair neocloud infrastructure with physical AI applications. Physical AI embeds intelligence directly into real-world environments, turning ordinary machines into smart systems. In an industrial environment, for example, I recently toured a mid-sized manufacturing plant that had deployed edge AI for quality control. Their system catches microscopic defects that would slip past human inspectors—all without sending sensitive production data to the cloud. The factory manager showed me how their autonomous mobile robots serve as edge computing platforms, processing data locally while feeding only aggregated insights to central systems. What struck me was how accessible this technology has become. Three years ago, this setup would have required millions in infrastructure. Today, modular neocloud units make it possible for even modest operations. In another sector, healthcare providers face similar barriers, but the stakes involve human lives. A rural hospital director demonstrated how their edge AI systems analyze medical images locally, identifying potential issues before a radiologist even opens the file. For remote facilities without specialist physicians on site, this capability is game-changing. The same hospital uses Nvidia’s Isaac for Healthcare platform to enable diagnostic capabilities previously unavailable outside major medical centers. “We’re getting urban-level diagnostics in a facility where the nearest city is three hours away,” the director told me. Implementation: getting started without breaking the bank For organizations looking to leverage these technologies, I recommend a three-step approach: First, conduct an honest workload assessment. Neocloud infrastructure works best when utilization remains consistently high. Map up your actual computing needs before committing. Second, strategically place your edge deployments. One manufacturing client embedded modular units directly on their factory floor, eliminating latency issues that had plagued their quality control system. Finally, prioritize security and privacy. One of the overlooked benefits of edge computing is keeping sensitive data local. A healthcare CISO I interviewed noted: “When patient data never leaves our facility, our compliance burden drops significantly.” What is next? The democratization of AI infrastructure through neoclouds represents more than cost savings—it is unlocking innovation previously monopolized by tech giants. For the first time, small enterprises, startups and specialized applications have a fighting chance to implement transformative AI solutions. Whether it is smarter factories, more accessible healthcare or entirely new applications we have not imagined yet, the barrier to entry has fundamentally changed. The real question is not whether your organization can afford custom AI anymore—it is whether you can afford to be left behind. — Durgesh Srivastava is co-founder and CTO of DataraAI. He was previously CTO at MIPS, and prior to that at Nvidia, where he led the architecture of the Grace platform, NVLink interconnects, and advanced thermal and power systems. His earlier work at Intel included multiple Xeon generations and automotive tech. RELATED TOPICS: AI AND BIG DATA , CLOUD COMPUTING Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "A two-way Wilson current mirror",
    "source": "EDN Network",
    "date": "2025-06-11",
    "link": "https://www.edn.com/a-two-way-wilson-current-mirror/",
    "text": "Recently (5/21/25), I published a Design Idea (DI) describing a bidirectional current mirror using just two transistors and a diode, “ A two-way mirror—current mirror that is .” That circuit works well in some applications, like the one shown in the DI itself, but it nevertheless has limitations due to various error-introducing effects, some of which are described in this reference . One of the more important factors of these is the “ Early effect”, that kicks in to cause significant current imbalance when the voltage drop across the mirror becomes more than a couple of volts. Happily, a fix for the Early effect was devised back in 1967 by George R. Wilson, an IC design engineer at Tektronix. Wilson’s simple yet ingenious fix consisted of adding a third transistor to the basic current mirror, Q3 in Figure 1 .  Figure 1 The Wilson mirror improves accuracy despite Early -effect by adding Q3.  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 Wow the engineering world with your unique design: Design Ideas Submission Guide Due to Q3, the difference in voltage drop across the active mirror transistors Q1 and Q2 never differs by more than one Vbe, which reduces Early effect error to less than 1%. Figure 2 shows a variation of the basic two-sided current mirror incorporating Wilson’s accuracy-enhancing extra transistor. Figure 2 A two-way current mirror with Wilson enhancement. This advantage is exploited in the Figure 3 triangle-wave oscillator to achieve good T-wave symmetry despite as much as 10v being applied across the mirror. Figure 3 D1, Q3, Q4, and Q5 comprise the two-way Wilson current mirror. It passively conducts Q2’s collector current to C1 via Q3’s base-collecter when OUT versus Vc1 is negative, but becomes an active inverting unity-gain current source when OUT goes positive and reverses the polarity. Keep variable Vin < 4v so Q2 won’t saturate on negative output peaks. The Wilson mirror is the better choice for Figure 3, but it isn’t always the superior option. That’s because the extra transistor adds an extra Vbe to the minimum voltage drop when the mirror is active. This is no problem for Figure 3 because of its operation from 15v, but would be a deal breaker in the previous DI with its 5v-powered oscillator. Luckily, for the same reason, Early already wasn’t much of an issue there. Courses for horses? Stephen Woodward ’s relationship with EDN’s DI column goes back quite a long way. Over 100 submissions have been accepted since his first contribution back in 1974. Related Content A two-way mirror—current mirror that is Use a current mirror to control a power supply A comparison between mirrors and Hall-effect current sensing Current Mirrors with a DCP offer precision and current gain control LED strings driven by current source/mirror Current mirror drives multiple LEDs from a low supply voltage 2 comments on “ A two-way Wilson current mirror ” asa70 June 11, 2025 Nice ! Inspired ! A necessary enhancement, it will make slopes much more balanced and linear, especially since pnp transistors are much poorer as regards early effect. Wilson really improves Ro by maybe 50 times ! Any particular reason to use the 5087/5089 as against other complementary pairs? Been meaning to comment since your 10 octave idea but can’t find any time, but HAD to steal some today for a brief comment. Hope to comment in more detail soon. best, Ashutosh Log in to Reply WSWoodward June 11, 2025 “Inspired !” Thanks! Better to be inspired than expired, I always say! “Any particular reason to use the 5087/5089…? 1. They’re old friends. 2. Good betas (the ’09 typicals at 1k). Log in to Reply Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "China Sharpens Strategy in the Global 6G Race",
    "source": "EE Times",
    "date": "2025-06-10",
    "link": "https://www.eetimes.com/china-sharpens-strategy-in-the-global-6g-race/",
    "text": "design Lines AI & Big Data Designline China Sharpens Strategy in the Global 6G Race China’s 6G strategy blends global standards participation with self-reliance, aiming for technology leadership. By Pablo Valerio 06.10.2025 0 Share Post Share on Facebook Share on Twitter China is investing heavily on 6G technology, yet its strategy confronts obstacles stemming from a fragmented global standards environment. The global telecommunications sector is poised for a new era with the imminent arrival of sixth-generation (6G) cellular networks. Promising capabilities vastly exceeding 5G, including terabit-per-second speeds, sub-millisecond latencies, and a seamless fusion of intelligence and sensing, 6G is poised to reshape connectivity and drive innovation across industries. Against this backdrop of ambitious technological aspiration and the promise of a deeply interconnected, intelligent world, a central narrative is emerging: China’s determined push to secure leadership in this critical future technology. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 The AI wireless revolution While the target launch is around 2030, the contours of 6G, officially designated “IMT-2030” by the International Telecommunication Union (ITU), are already being defined. Not merely as an incremental speed boost over 5G but rather as a paradigm shift. The performance targets are exceptionally ambitious. They aim for peak data rates up to a terabit per second, air-interface latencies as low as 100 microseconds, and support for up to 10 million devices per square kilometer. Source: International Telecommunication Union ( ITU ) Crucially, 6G networks will natively integrate Artificial Intelligence (AI), sensing, and Non-Terrestrial Networks (NTN) from the outset, marking a significant architectural departure from 5G, where such features appeared in later releases. Key enabling technologies identified as fundamental to achieving these goals include the utilization of Terahertz (THz) and Sub-THz frequencies for massive bandwidth, AI-native networks for dynamic optimization, Integrated Sensing and Communication (ISAC) capabilities, advanced Multiple-Input Multiple-Output (MIMO) and antenna technologies, and Reconfigurable Intelligent Surfaces (RIS) to manage radio wave propagation. The sheer diversity and extreme nature of these targets mean 6G is being designed as a highly flexible platform, tailored for a broad spectrum of specialized applications, from enhanced mobile broadband and ultra-reliable low-latency communications to massive machine-type communication, alongside entirely new scenarios like pervasive intelligence and integrated sensing. Road to global 6G standard The formal journey toward 6G standardization is a multi-year global effort coordinated primarily by the ITU and executed through detailed technical specifications developed by the 3rd Generation Partnership Project (3GPP). The ITU-R defines the overall framework and performance requirements for IMT-2030. The call for candidate technologies was issued in October 2024, with submissions expected between 2027 and early 2029, leading to the final IMT-2030 designation anticipated by 2030. 3GPP officially committed to 6G specifications in late 2023. Release 19 , currently underway, includes preliminary studies for 6G. The upcoming Release 20 is the main study phase, with Release 21 intended to contain the first implementable 6G specifications, expected by the end of 2028. Source: 3GPP This phased approach allows for the continued evolution of 5G-Advanced while laying the groundwork for 6G, though it presents challenges in resource allocation and avoiding early path dependency. A notable shift from 5G is the push to link 6G capabilities directly to clear business cases and monetization strategies from the design phase. Key architectural principles guiding 6G include the need for a single, global standard, support for wide spectrum ranges, efficient multi-RAT spectrum sharing (MRSS), a smoother transition from 5G (likely evolving the 5G Core), exclusive Stand-Alone (SA) operation, open interfaces where valuable, native observability for AI/ML, and a continued focus on energy efficiency. China’s political ambition and strategy The idea of evolving the 5G Core for 6G is not universally accepted. Some in the Chinese telecom ecosystem advocate for a completely new 6G core network, a significant point of potential contention. China has unequivocally declared 6G a national strategic priority . It aims for commercialization by approximately 2030 and standard-setting around 2025. The strategy derives from a state-led approach that efficiently directs national resources, influences domestic market development, and strategically seeks to shape global standards. Initiatives like the “China Standards 2035” project underscore a long-term pivot towards defining the rules and frameworks for emerging technologies like 6G, moving from primarily a producer to a global standard-setter. The IMT-2030 (6G) Promotion Group is China’s central coordinating body. It fosters collaboration across academia, industry, and government, actively contributing to international standardization efforts. China’s commitment is also visible in proactive measures, such as launching the world’s fir st experimental 6G satellite in November 2020 to test THz signal transmission and, notably, allocating a significant portion of the 6 GHz band (6425-7125 MHz) specifically for 5G and future 6G services. Beijing views this 6 GHz spectrum as the “only high-quality resource with large bandwidth in the mid-band,” and its early allocation to stabilize domestic industry expectations and potentially influence global spectrum harmonization decisions. From national champions to global leaders China’s telecommunications giants are leading the move. Huawei, despite geopolitical pressures, remains a formidable cellular infrastructure provider. The company began 6G research as early as 2017 and continues to invest substantially , focusing on AI integration, ultra-low latency, massive IoT, and integrated sensing. Huawei pavilion at MWC Barcelona (EETimes) Huawei has a significant portfolio of 6G-related patents , leveraging its extensive networking, software, and mobile communications IP. Its vision for 6G is ambitious, defining it as a “distributed neural network” that fuses physical, biological, and cyber worlds, built on pillars like Native AI, Networked Sensing, and Integrated NTN. Huawei’s efforts, including developing its own AI chips, demonstrate a clear strategy toward technological self-reliance. ZTE is another major Chinese vendor engaged in 6G R&D and standardization. ZTE’s strategy heavily emphasizes AI-driven end-to-end network solutions, including its “AIR DNA” concept and detailed architectural proposals, such as research into the Network Data Analytics Function (NWDAF) for 6G. Alongside these Chinese champions, European vendors like Ericsson and Nokia are key players in the global 6G landscape. Ericsson is deeply involved in 6G research , contributing to architectural principles and collaborating on technologies like sub-THz communication with Intel. Ericsson also has aspirations for patent leadership in the 6G era. MWC 2025 (Credit: Nokia) Nokia is another leading vendor working on system architecture , sub-THz research, network automation, and sensing, participating in initiatives like the European Hexa-X projects. 3GPP and potential friction for different standards The 3GPP remains crucial for defining the detailed technical specifications that will underpin 6G. Chinese companies are major contributors, actively participating in technical working groups. Their goal is to influence the direction of standards and ensure their patented technologies become essential components. However, fundamental disagreements exist within the 3GPP. For example, at the March 2025 6G workshop , “many in the Chinese telecom ecosystem” advocated for a completely new 6G core network , contrasting with others who preferred an evolution of the 5G Core. Such architectural debates could become “critical fault lines” if consensus proves elusive. The “ Joint Statement Endorsing Principles for 6G : Secure, Open, and Resilient by Design ” adds another layer of complexity. It was signed by ten nations, including the US, key EU members, Japan, and South Korea. This statement, emphasizing security and openness, is an effort by like-minded nations to collectively influence standards and create a counterweight to regions with potentially differing priorities for 6G governance. Are we going to a fractured 6G? The critical question is whether China’s leadership pursuit will significantly diverge from global 6G standards. While a “strong impetus for a unified global 6G standard exists, driven by economic benefits and the need for interoperability”, significant geopolitical tensions and differing national priorities create a tangible risk of divergence. Arguments for a unified standard include the substantial economic benefits of economies of scale and the need for global interoperability and roaming. China’s active participation in ITU and 3GPP also suggests a preference to shape the standard from within. However, powerful drivers push towards an independent path. The primary impetus is the pursuit of technological sovereignty and self-reliance , mainly as a reactive measure to perceived Western efforts to constrain China’s technological rise. Furthermore, national security concerns drive the desire to control critical infrastructure with trusted domestic technology. There’s also the potential to embed national values and governance models into a distinct standard. A complete decoupling into two entirely separate standards appears less probable than a scenario where China champions a 6G variant with distinct features aligned with its national interests. This could potentially lead to a “fractured” rather than a cleanly bifurcated global 6G ecosystem. The divergence could manifest through regional variations, proprietary extensions, or mandated profiles within a broader IMT-2030 framework. Unity or division? Should significant divergence occur, the consequences would be profound. Economically, fragmented markets would reduce economies of scale, leading to higher costs for industry and consumers. R&D and deployment expenses would increase as companies support multiple versions of technology. Geopolitically, it would solidify techno-economic blocs and intensify competition for influence, particularly over developing nations. Technologically, it would severely hamper global interoperability and slow the overall pace of innovation. China’s calculated path China could become a dominant force in 6G. Its strategy is multifaceted: it actively participates in and seeks to shape global standards through bodies like the ITU and 3GPP while simultaneously building indigenous capabilities for significant self-reliance. While a unified global standard is preferred for economic reasons, China is proactively preparing for a scenario where it might need to rely more heavily on its own technology and champion a 6G ecosystem with distinctly Chinese characteristics. The degree of divergence will ultimately form by the evolving geopolitical landscape and dynamics within the global standards bodies. Stakeholders worldwide must prepare for a future 6G landscape that may be less monolithic, requiring greater adaptability and strategic foresight. RELATED TOPICS: 6G , AI , CHINA , WIRELESS Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn Pablo Valerio Pablo is a seasoned engineer with 30+ years of experience. For over 10 years, he's been a contributing editor for EE Times (where he edits the Supply Chain section). He also wrote for EPSNews, InformationWeek, EBN, LightReading, Network Computing, and IEEE Xplore. His coverage spans Supply Chain, Semiconductors, Networks, IoT, Security, and Smart Cities. He holds an MEng, Electrical and Electronics Engineering from The Ohio State University. Follow Pablo on LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "MEMS Speakers Enable Full-Range Audio in Wearables with Ultrasound",
    "source": "EE Times",
    "date": "2025-06-10",
    "link": "https://www.eetimes.com/mems-speakers-enable-full-range-audio-in-wearables-with-ultrasound/",
    "text": "design Lines Signal Processing Designline MEMS Speakers Enable Full-Range Audio in Wearables with Ultrasound By Mike Housholder, VP Marketing and Business Development, xMEMS Labs 06.10.2025 0 Share Post Share on Facebook Share on Twitter It can be easy to forget: There is a tiny loudspeaker in your smart watch. Not to mention in your smart glasses, too. As we move toward a world of always-available AI assistants, glasses and watches will become a primary alternative to earbuds as a means for interfacing with AI on-the-go—ears-free and hands-free. But the current state-of-the-art smart glasses and watches do not match conventional glasses and watches in terms of style, thinness and weight, which will limit broad adoption. One key limiting factor is the “micro” speakers in today’s smart electronics must fit the weight, style and form factor of their devices—and that is not easy. Traditionally, speakers need space—even just a little—to create sound louder than beeps and alerts. At the micro level, they barely fit the devices they go into, limiting design and functionality. There is a reason so many smart glasses sport a more “Wayfarer” style: The thicker frames are needed to house today’s micro speakers, as well as other electronics. But that could potentially change with the advent of tiny, ultralightweight, all-silicon micro speakers. To bring these even smaller speakers to consumers as well as a wider variety of smart electronics, that requires reimagining how a micro speaker works. Partner Content View All GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! By GigaDevice Semiconductor Inc. 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness By ASPEED Technology Inc 06.12.2025 The Opportunity for Liquid Sensing By Infineon Technologies 06.05.2025 Coils are a constraining factor The challenge until now with designing audio into electronic gadgets like smart watches, glasses, AR headsets and other emerging devices is how to make them sleek and fashion-forward. The added dilemma is that speakers that sound good add bulk. Most existing micro speakers are based on a century-old dynamic-driver, coil and magnet-based form factor. Getting them thin enough to fit current-generation watches or glasses, about 3-mm thick, means significant tradeoffs—especially in sound quality. Even at 3 mm, such a micro speaker hinders design flexibility. What electronics makers need, however, is a micro speaker that is as small as technologically possible yet still delivers rich sound; a speaker not constrained by the size and weight of coils and magnets. More specifically, a MEMS speaker manufactured from silicon in high-capacity wafer fabs. MEMS speaker prototypes have been around for years from several suppliers, but only a few have made it into production, and even fewer have achieved high volume. Most MEMS speakers utilize conventional push-air, piston-like transduction, similar to coil speakers. But the limited displacement of the MEMS when compared to coil limits their sound output. As a result, most MEMS speakers are in service only in a limited capacity as a high-frequency tweeter in in-ear wireless earbuds or glasses. The thought of achieving full frequency range output in-ear or in free-air as required by glasses and watches is out of reach for most MEMS speakers. A novel reinvention of the audio transduction mechanism, one that is not dependent on large displacement, is required to achieve high-quality, full-range sound in free-air. Again, not easy. Such a tiny MEMS speaker must be able to generate enough sound pressure to operate in open air between the smart device and the listener’s ears while at the same time being tiny enough to integrate into any product. Yes, the goal is to generate big sound from the smallest possible speaker. A speaker so small that manufacturers can imagine any design with confidence, because full-range audio is a given. Solid-state micro speakers that can make sound from ultrasound Leading audio engineers recently developed a 1-mm-thin, open-air, near-field MEMS loudspeaker that creates sound from ultrasonic air pulses, rather than from a piston coil and magnet structure. Because it is a semiconductor MEMS speaker, by its very nature as a microchip it can enhance advanced DSP functions like privacy and transparency modes with improved, more consistent part-to-part performance that is not possible with legacy coil architectures. What it does is turn ultrasonic pulses that are otherwise imperceptible to human ears into full-bandwidth, high-fidelity audio. The open-air MEMS speaker is one-third the thickness — and one-seventh the overall size — of existing dynamic-driver designs, making it an optimal fit inside a growing number of thin-and-stylish wearable devices. (Image: xMEMS) The open-air MEMS speaker is one-third the thickness—and one-seventh the overall size—of existing dynamic-driver designs, making it a good fit inside a growing number of thin-and-stylish wearable devices. The fact that it is a solid-state speaker (i.e., no moving parts), it is robust and IP58-rated against water and dust makes it a key requirement for wearable electronics. It has been tested in smart glasses, smart watch and open-fit earphone (OWS) form factors and has achieved equal or louder sound pressure levels (SPL) at both high and low frequencies, with significantly better bass performance (delivering 11dB greater SPL at 40Hz and about 15dB more at 5KHz than leading OWS earphones). A solid-state design means a faster response than coil-based speakers and near-zero phase shift, so that the full-range audio it produces is also clear, detailed and accurate. The part-to-part consistency creates near-perfect phase coherency between left and right speakers, which should enhance DSP features like privacy mode in smart glasses and OWS earbuds to limit sound leakage to those nearby. Other uses for solid-state MEMS speakers Open-air, solid-state MEMS speakers have other uses, as well. It is especially a challenge to generate robust, high-frequency sound in a near-field micro speaker. Because the open-air loudspeaker design can generate at least 90dB at the highest frequencies, it is well suited to “tweeter”-style applications. For example, as a smaller, lighter earpiece speaker in smartphones. We consider the MEMS, near-field micro speaker that performs in open air, rather than in closed, occluded earbuds, a major innovation. It helps overcome design challenges in creating lightweight, wearable smart devices , helping electronics manufacturers give consumers sleek product designs coupled with high quality audio experiences. RELATED TOPICS: MEMS & SENSORS Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Click to share on LinkedIn (Opens in new window) LinkedIn 0 comments Post Comment Leave a Reply Cancel reply You must Register or Login to post a comment. This site uses Akismet to reduce spam. Learn how your comment data is processed ."
  },
  {
    "title": "A two-wire temperature transmitter using an RTD sensor",
    "source": "EDN Network",
    "date": "2025-06-10",
    "link": "https://www.edn.com/a-two-wire-temperature-transmitter-using-an-rtd-sensor/",
    "text": "Designing an analog circuit can be a frustrating experience, as noted by Nick Cornford in his Design Idea (DI), “ DIY RTD for a DMM .” With a fortnight’s struggle, I completed the design of 4 mA to 20mA, two-wire temperature transmitter using a platinum resistance temperature detector (RTD) sensor, as shown in Figure 1 . Figure 1 A two-wire transmitter with a platinum RTD sensor, R4. R1 and R4 will determine the currents through the respective limbs; these currents must be kept to 0.5 mA. The circuit is designed to operate from 0 °C to 300 °C, where R5 can be adjusted to change this temperature range. Wow the engineering world with your unique design: Design Ideas Submission Guide  In industrial processing applications, a two-wire topology is used to connect the temperature sensor in the field to the control room equipment, such as a distributed control system (DCS), programmable logic controller (PLC), or indicator. A 24-V DC supply is fed from the control room, and the current drawn is proportional to the temperature. Since the power and signal travel in the same pair of wires, this arrangement offers cable savings.  Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 In Figure 1’s circuit, R4 is an RTD. As per the platinum RTD’s temperature versus resistance table (DIN EN 60751), R4 is 100 Ω at 0 °C and 212.2 Ω at 300 °C. This circuit is designed for the range of 0 °C to 300 °C, where the load current will be 4 mA for 0 °C and 20 mA for 300 °C (you may change R5 to achieve other ranges). The current is proportional to the resistance of the RTD, which is slightly non-linear with respect to temperature. The accuracy claimed in this circuit is ±1%, which is adequate for many applications. To avoid self-heating of the RTD, only 0.5 mA is sent through it. R1 and R2 must be adjusted to pass 0.5 mA in each limb. U1 and U4 are wired as zero tempco current sources. The difference in voltage between RTD (R4) and R3 is amplified in the instrumentation amplifier U3. The RTD at 300 °C (or R4 at 212.2 Ω) sends 160 µA through R8. R10 sets the current through it as 40 µA. Hence, a total of 200 µA is sent as the input to the transmitter IC, U5. U5 then amplifies this current by 100 to 20 mA and converts it into a two-wire format. The U2 circuit generates -3.3 VDC to feed the negative supply voltage pin of U3. Accurate results were only achieved when operating U3 with a dual supply. The RTD at 0 °C gives a 4-mA current at the output (LOAD). Jayapal Ramalingam has over three decades of experience in designing electronics systems for power & process industries and is presently a freelance automation consultant. Related Content DIY RTD for a DMM Two-wire interface has galvanic isolation Two-wire remote sensor preamp Designing with temperature sensors, part three: RTDs 0 comments on “ A two-wire temperature transmitter using an RTD sensor ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  },
  {
    "title": "Tony Pialis’ design journey to high-speed SerDes",
    "source": "EDN Network",
    "date": "2025-06-10",
    "link": "https://www.edn.com/tony-pialis-design-journey-to-high-speed-serdes/",
    "text": "Alphawave Semi is in the news after being acquired by Qualcomm for $4.2 billion, and so is its co-founder and CEO Tony Pialis, now widely seen as a semiconductor connectivity IP veteran. Here is a brief profile of Pialis, highlighting how the design of analog and mixed-signal semiconductors fascinated him early in his career and how this led to his work on DSP-centric SerDes architectures. Read the full story at EDN ’s sister publication, Planet Analog . Related Content  IP players prominent in chiplet’s 2024 diary Alphawave Semi’s quest for open chiplet ecosystem BoW Strengthens Pathway to Chiplet Standardization How the Worlds of Chiplets and Packaging Intertwine Why UCIe is Key to Connectivity for Next-Gen AI Chiplets Partner Content GD32C231 Series MCU — Redefining Cost-Performance, Unleashing New Potential! 06.12.2025 Cupola360 RX2000: Smart Patrol Camera at COMPUTEX for Real-Time Spatial Awareness 06.12.2025 The Opportunity for Liquid Sensing 06.05.2025 0 comments on “ Tony Pialis’ design journey to high-speed SerDes ” Leave a Reply Cancel reply You must Sign in or Register to post a comment."
  }
]