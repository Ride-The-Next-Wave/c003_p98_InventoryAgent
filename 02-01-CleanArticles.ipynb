{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Clean Articles\n",
    "This notebook cleans the article data in preparation for modeling. The ultimate goal is to match cleaned articles to the products based on their provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON file\n",
    "with open(\"./data/article-data-raw.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define text cleaning functions\n",
    "def clean_html(text):\n",
    "    return BeautifulSoup(str(text), \"html.parser\").get_text()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)           # normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)        # remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Parse and clean each article while preserving metadata\n",
    "def parse_article(text, metadata):\n",
    "    # Combine text and metadata into one dictionary\n",
    "    return {\n",
    "        \"title\": metadata.get(\"title\", \"\"),\n",
    "        \"author\": metadata.get(\"author\", \"\"),\n",
    "        \"date\": metadata.get(\"date\", \"\"),\n",
    "        \"url\": metadata.get(\"url\", \"\"),\n",
    "        \"content\": text.strip()\n",
    "    }\n",
    "\n",
    "# Combine each document with its corresponding metadata\n",
    "if \"documents\" in data and isinstance(data[\"documents\"], list):\n",
    "    documents = data[\"documents\"]\n",
    "    metadatas = data.get(\"metadatas\", [])\n",
    "\n",
    "    if len(documents) != len(metadatas):\n",
    "        raise ValueError(\"Mismatch between number of documents and metadata entries\")\n",
    "\n",
    "    cleaned_articles = [parse_article(doc, meta) for doc, meta in zip(documents, metadatas)]\n",
    "else:\n",
    "    raise ValueError(\"JSON does not contain 'documents' key with a list\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "We save the cleaned article data as a json file for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./intermediate_data/article-data-clean.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_articles, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "After you saved the dataset here, run the next step in the workflow [02-02-CleanProducts.ipynb](./02-02-CleanProducts.ipynb) or go back to [00-Workflow.ipynb](./00-Workflow.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Authors:**\n",
    "[Salah Mohamoud](mailto:salah.mohamoud.dev@gmail.com),\n",
    "[Sai Keertana Lakku](mailto:saikeertana005@gmail.com),\n",
    "[Zhen Zhuang](mailto:zhuangzhen17cs@gmail.com),\n",
    "[Nick Capaldini](mailto:nick.capaldini@ridethenextwave.com), Ride The Next Wave, May 19, 2025\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
