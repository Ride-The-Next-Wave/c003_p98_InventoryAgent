{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Clean Products List\n",
    "\n",
    "This notebook cleans the prooduct raw data in preparation for modeling. The ultimate goal is to match cleaned articles to the products based on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./intermediate_data/Products_List_Raw.json', 'r') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "def extract_product_info(element):\n",
    "    try:\n",
    "        name_elem = element.find(['a', 'span', 'div'], text=True)\n",
    "        if name_elem:\n",
    "            name = name_elem.get_text(strip=True)\n",
    "            if len(name) > 5:\n",
    "                return {\n",
    "                    'name': name,\n",
    "                    'description': name,\n",
    "                    'part_number': None,\n",
    "                    'quantity_available': None\n",
    "                }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def scrape_category_products(category_url):\n",
    "    try:\n",
    "        response = requests.get(category_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = []\n",
    "        elements = soup.find_all(['div', 'tr'], class_=re.compile(r'product|item|row', re.I))\n",
    "\n",
    "        for el in elements[:100]:\n",
    "            product = extract_product_info(el)\n",
    "            if product:\n",
    "                products.append(product)\n",
    "        \n",
    "        print(f\"Found {len(products)} products\")\n",
    "        return products\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "all_products = []\n",
    "for i, cat in enumerate(categories[:20]): \n",
    "    products = scrape_category_products(cat['url'])\n",
    "    for product in products:\n",
    "        product['category'] = cat['name']\n",
    "        product['main_category'] = cat['category']\n",
    "        all_products.append(product)\n",
    "    time.sleep(2)\n",
    "\n",
    "results = {\n",
    "    'categories': categories,\n",
    "    'products': all_products,\n",
    "    'summary': {\n",
    "        'total_categories': len(categories),\n",
    "        'total_products': len(all_products)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Product List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./intermediate_data/Products_List_Clean.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
