{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Digi-Key product scraping...\n",
      "Scraping main categories from: https://www.digikey.com/en/products\n",
      "Found 681 unique categories\n",
      "\n",
      "Results saved to: digikey_product_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DigiKeyProductScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Referer\": \"https://www.digikey.com/\",\n",
    "        }\n",
    "        self.base_url = \"https://www.digikey.com\"\n",
    "        self.product_data = []\n",
    "\n",
    "    def scrape_main_categories(self):\n",
    "        \"\"\"Scrape the main product index page to get categories and subcategories\"\"\"\n",
    "        try:\n",
    "            url = \"https://www.digikey.com/en/products\"\n",
    "            print(f\"Scraping main categories from: {url}\")\n",
    "            \n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for category sections\n",
    "            categories = []\n",
    "            \n",
    "            # Look for category links\n",
    "            category_links = soup.find_all('a', href=True)\n",
    "            \n",
    "            for link in category_links:\n",
    "                href = link.get('href', '')\n",
    "                text = link.get_text(strip=True)\n",
    "                \n",
    "                # Filter for product category links\n",
    "                if '/en/products/' in href and text:\n",
    "                    clean_name = self.clean_category_name(text)\n",
    "                    \n",
    "                    if clean_name and len(clean_name) > 3:  # Filter out short/invalid names\n",
    "                        categories.append({\n",
    "                            'name': clean_name,\n",
    "                            'url': self.base_url + href if not href.startswith('http') else href,\n",
    "                            'category': self.determine_main_category(clean_name)\n",
    "                        })\n",
    "            \n",
    "            # Remove duplicates\n",
    "            seen_urls = set()\n",
    "            unique_categories = []\n",
    "            for cat in categories:\n",
    "                if cat['url'] not in seen_urls:\n",
    "                    seen_urls.add(cat['url'])\n",
    "                    unique_categories.append(cat)\n",
    "            \n",
    "            print(f\"Found {len(unique_categories)} unique categories\")\n",
    "            return unique_categories\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping main categories: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clean_category_name(self, text):\n",
    "        \"\"\"Clean category name by removing item counts\"\"\"\n",
    "        cleaned = re.sub(r'\\s*[\\d,]+\\s+Items?.*$', '', text, flags=re.IGNORECASE)\n",
    "        return cleaned.strip()\n",
    "\n",
    "    def determine_main_category(self, name):\n",
    "        \"\"\"Determine main category based on product name\"\"\"\n",
    "        name_lower = name.lower()\n",
    "        if any(term in name_lower for term in ['anti-static', 'esd', 'clean room']):\n",
    "            return 'Anti-Static, ESD, Clean Room Products'\n",
    "        elif any(term in name_lower for term in ['audio', 'microphone', 'speaker', 'amplifier']):\n",
    "            return 'Audio Products'\n",
    "        elif any(term in name_lower for term in ['battery', 'batteries']):\n",
    "            return 'Battery Products'\n",
    "        elif any(term in name_lower for term in ['cable', 'wire', 'connector']):\n",
    "            return 'Cables & Connectors'\n",
    "        elif any(term in name_lower for term in ['capacitor']):\n",
    "            return 'Capacitors'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "    def scrape_category_products(self, category_url, max_pages=5):\n",
    "        \"\"\"Scrape individual products from a category page\"\"\"\n",
    "        try:\n",
    "            print(f\"Scraping products from: {category_url}\")\n",
    "            \n",
    "            response = requests.get(category_url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            products = []\n",
    "            \n",
    "            # Look for product listings\n",
    "            product_elements = soup.find_all(['div', 'tr'], class_=re.compile(r'product|item|row', re.I))\n",
    "            \n",
    "            for element in product_elements[:100]:  # Limit to first 100 products per category\n",
    "                product_info = self.extract_product_info(element)\n",
    "                if product_info:\n",
    "                    products.append(product_info)\n",
    "            \n",
    "            print(f\"Found {len(products)} products in this category\")\n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping category {category_url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_product_info(self, element):\n",
    "        \"\"\"Extract individual product information from HTML element\"\"\"\n",
    "        try:\n",
    "            name_elem = element.find(['a', 'span', 'div'], text=True)\n",
    "            if name_elem:\n",
    "                name = name_elem.get_text(strip=True)\n",
    "                if len(name) > 5:\n",
    "                    return {\n",
    "                        'name': name,\n",
    "                        'description': name, \n",
    "                        'part_number': None, \n",
    "                        'quantity_available': None \n",
    "                    }\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    def run_full_scrape(self, include_products=False):\n",
    "        \"\"\"Run the complete scraping process\"\"\"\n",
    "        print(\"Starting Digi-Key product scraping...\")\n",
    "        \n",
    "        # Get all categories\n",
    "        categories = self.scrape_main_categories()\n",
    "        if not categories:\n",
    "            print(\"No categories found. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # Scrape products from each category\n",
    "        if include_products:\n",
    "            for i, category in enumerate(categories[:20]):  # Limit to first 10 categories\n",
    "                print(f\"\\nScraping category {i+1}/{min(10, len(categories))}: {category['name']}\")\n",
    "                products = self.scrape_category_products(category['url'])\n",
    "                \n",
    "                for product in products:\n",
    "                    product['category'] = category['name']\n",
    "                    product['main_category'] = category['category']\n",
    "                    self.product_data.append(product)\n",
    "                \n",
    "                time.sleep(2) # the delays\n",
    "        \n",
    "        # Save results\n",
    "        self.save_results(categories)\n",
    "        \n",
    "        return categories\n",
    "\n",
    "    def save_results(self, categories):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        results = {\n",
    "            'categories': categories,\n",
    "            'products': self.product_data,\n",
    "            'summary': {\n",
    "                'total_categories': len(categories),\n",
    "                'total_products': len(self.product_data)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('./intermediate_data/digikey_product_data.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: digikey_product_data.json\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = DigiKeyProductScraper()\n",
    "    \n",
    "    #Get categories and counts\n",
    "    categories = scraper.run_full_scrape(include_products=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation_libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
