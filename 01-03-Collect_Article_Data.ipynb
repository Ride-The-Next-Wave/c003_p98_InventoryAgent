{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Retreive Article Data\n",
    "\n",
    "This notebook scrapes article links and retreives text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "import random\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import urllib3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "import cloudscraper\n",
    "import httpx\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress SSL warning and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List extreme domains and proxy list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problematic domains that need extreme methods\n",
    "EXTREME_DOMAINS = ['www.eetimes.com', 'www.edn.com']\n",
    "\n",
    "# Free proxy list\n",
    "PROXY_LIST = [\n",
    "    # Add working proxies here if available\n",
    "    # 'http://proxy1:port',\n",
    "    # 'http://proxy2:port',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloudscraper_session():\n",
    "    try:\n",
    "        scraper = cloudscraper.create_scraper(\n",
    "            browser={\n",
    "                'browser': 'chrome',\n",
    "                'platform': 'windows',\n",
    "                'desktop': True\n",
    "            }\n",
    "        )\n",
    "        return scraper\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create cloudscraper: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_selenium_driver():\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')  # Run in background\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create Selenium driver: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_with_retries():\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "            backoff_factor=1\n",
    "        )\n",
    "    except TypeError:\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            backoff_factor=1\n",
    "        )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract article text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(html_content, url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Remove unwanted elements\n",
    "    for script in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\", \"form\", \"iframe\", \"noscript\"]):\n",
    "        script.decompose()\n",
    "    selectors = [\n",
    "        'article',\n",
    "        '[role=\"main\"]',\n",
    "        '.article-content',\n",
    "        '.post-content',\n",
    "        '.entry-content',\n",
    "        '.content',\n",
    "        '.main-content',\n",
    "        '#content',\n",
    "        '.article-body',\n",
    "        '.story-body',\n",
    "        '.post-body',\n",
    "        '.article-text',\n",
    "        '.body-content',\n",
    "        '.article-wrapper'\n",
    "    ]\n",
    "    text_content = \"\"\n",
    "\n",
    "    # Process each selector\n",
    "    for selector in selectors:\n",
    "        elements = soup.select(selector)\n",
    "        if elements:\n",
    "            for element in elements:\n",
    "                text_content += element.get_text(separator=' ', strip=True) + \" \"\n",
    "            break\n",
    "    # Fallback to paragraphs\n",
    "    if not text_content.strip():\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30])\n",
    "    # Last resort - get body text\n",
    "    if not text_content.strip():\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            text_content = body.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    return text_content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_with_requests(url, session):\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'\n",
    "    ]\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Pragma': 'no-cache'\n",
    "    }\n",
    "    response = session.get(url, headers=headers, timeout=(15, 45), verify=False, allow_redirects=True)\n",
    "    response.raise_for_status()\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_cloudscraper(url, scraper):\n",
    "    if scraper is None:\n",
    "        raise Exception(\"CloudScraper not available\")\n",
    "    response = scraper.get(url, timeout=45)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "async def scrape_with_httpx(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=45.0, verify=False) as client:\n",
    "        response = await client.get(url, headers=headers, follow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with selenium driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_selenium(url, driver):\n",
    "    if driver is None:\n",
    "        raise Exception(\"Selenium driver not available\")\n",
    "    driver.set_page_load_timeout(60)\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"p\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            pass  # Continue anyway\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "    time.sleep(2)\n",
    "    html_content = driver.page_source\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with httpx sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_httpx_sync(url):\n",
    "    \"\"\"Synchronous version of HTTPX scraping\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    with httpx.Client(timeout=45.0, verify=False) as client:\n",
    "        response = client.get(url, headers=headers, follow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme method to scrape an article (Uses 4 different methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_extreme(url, session=None, scraper=None, driver=None):\n",
    "    methods = []\n",
    "    if session:\n",
    "        methods.append((\"Requests\", lambda: scrape_with_requests(url, session)))\n",
    "    \n",
    "    if scraper:\n",
    "        methods.append((\"CloudScraper\", lambda: scrape_with_cloudscraper(url, scraper)))\n",
    "    methods.append((\"HTTPX\", lambda: scrape_with_httpx_sync(url)))\n",
    "    \n",
    "    if driver:\n",
    "        methods.append((\"Selenium\", lambda: scrape_with_selenium(url, driver)))\n",
    "    \n",
    "    for method_name, method_func in methods:\n",
    "        try:\n",
    "            html_content = method_func()\n",
    "            if html_content and len(html_content) > 1000:\n",
    "                article_text = extract_article_text(html_content, url)\n",
    "                if len(article_text) > 100:\n",
    "                    return article_text\n",
    "                else:\n",
    "                    logging.warning(f\"{method_name} got content but extraction failed\")\n",
    "            else:\n",
    "                logging.warning(f\"{method_name} returned insufficient content\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def is_extreme_domain(url):\n",
    "    return any(domain in url.lower() for domain in EXTREME_DOMAINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape all articles with extreme methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url, session=None, scraper=None, driver=None):\n",
    "    # For extreme domains, use all methods\n",
    "    if is_extreme_domain(url):\n",
    "        result = scrape_article_extreme(url, session, scraper, driver)\n",
    "        if result:\n",
    "            return result\n",
    "        else:\n",
    "            logging.error(f\"All extreme methods failed for {url}\")\n",
    "            return \"Content unavailable - all methods failed\"\n",
    "    # For normal domains, use regular method\n",
    "    try:\n",
    "        if session is None:\n",
    "            session = create_session_with_retries()\n",
    "        \n",
    "        html_content = scrape_with_requests(url, session)\n",
    "        article_text = extract_article_text(html_content, url)\n",
    "        \n",
    "        if len(article_text) > 100:\n",
    "            return article_text\n",
    "        else:\n",
    "            result = scrape_article_extreme(url, session, scraper, driver)\n",
    "            return result if result else \"Content unavailable - extraction failed\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        result = scrape_article_extreme(url, session, scraper, driver)\n",
    "        return result if result else f\"Content unavailable - {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect and organize article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_article_data(csv_path):\n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logging.info(f\"Loaded {len(df)} articles from CSV\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['title', 'url', 'date', 'source']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"Missing required columns: {missing_columns}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize all scraping tools\n",
    "    session = create_session_with_retries()\n",
    "    scraper = create_cloudscraper_session()\n",
    "    driver = create_selenium_driver()\n",
    "     \n",
    "    articles_data = []\n",
    "    successful_scrapes = 0\n",
    "    failed_scrapes = 0\n",
    "    try:\n",
    "        # Process each article\n",
    "        for index, row in df.iterrows():\n",
    "            logging.info(f\"Processing article {index + 1}/{len(df)}: {row['title'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                article_text = scrape_article(row['url'], session, scraper, driver)\n",
    "                \n",
    "                article_data = {\n",
    "                    'title': row['title'],\n",
    "                    'source': row['source'],\n",
    "                    'date': row['date'],\n",
    "                    'link': row['url'],\n",
    "                    'text': article_text,\n",
    "                }\n",
    "                \n",
    "                articles_data.append(article_data)\n",
    "                \n",
    "                if len(article_text) > 100 and not article_text.startswith(\"Content unavailable\"):\n",
    "                    successful_scrapes += 1\n",
    "                else:\n",
    "                    failed_scrapes += 1\n",
    "                    \n",
    "                # Progress update\n",
    "                if (index + 1) % 5 == 0:\n",
    "                    logging.info(f\"Progress: {index + 1}/{len(df)} - Success: {successful_scrapes}, Failed: {failed_scrapes}\")\n",
    "                \n",
    "                # Respectful delay\n",
    "                if is_extreme_domain(row['url']):\n",
    "                    time.sleep(random.uniform(2, 4))  # Longer for extreme domains\n",
    "                else:\n",
    "                    time.sleep(random.uniform(0.5, 1.5))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                article_data = {\n",
    "                    'title': row['title'],\n",
    "                    'source': row['source'],\n",
    "                    'date': row['date'],\n",
    "                    'link': row['url'],\n",
    "                    'text': f\"Processing error: {str(e)}\",\n",
    "                }\n",
    "                articles_data.append(article_data)\n",
    "                failed_scrapes += 1\n",
    "    finally:\n",
    "        # Clean up Selenium driver\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Final summary\n",
    "    logging.info(f\"Total articles processed: {len(articles_data)}\")\n",
    "    \n",
    "    return articles_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Save raw article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 12:11:31,877 - INFO - Loaded 30 articles from CSV\n",
      "2025-06-13 12:11:34,841 - INFO - Processing article 1/30: EnCharge Picks The PC For Its First Analog AI Chip...\n",
      "2025-06-13 12:11:39,855 - INFO - Processing article 2/30: IMS2025: Cross-correlation spectrum analyser from ...\n",
      "2025-06-13 12:11:41,270 - INFO - Processing article 3/30: Dragonwing modules support varied OS...\n",
      "2025-06-13 12:11:42,293 - INFO - Processing article 4/30: Tiger Lake-H Xeon-W SOSA single-board computer...\n",
      "2025-06-13 12:11:43,052 - INFO - Processing article 5/30: The 90nm Leakage Issue...\n",
      "2025-06-13 12:11:43,523 - INFO - Progress: 5/30 - Success: 5, Failed: 0\n",
      "2025-06-13 12:11:44,472 - INFO - Processing article 6/30: AMS Technologies to distribute Singular Photonics’...\n",
      "2025-06-13 12:11:45,670 - INFO - Processing article 7/30: Most Read – Qualcomm, Big Beautiful Bill, Semi sal...\n",
      "2025-06-13 12:11:47,022 - INFO - Processing article 8/30: Rugged circular connectors are latching, threaded ...\n",
      "2025-06-13 12:11:48,750 - INFO - Processing article 9/30: Green lights for SES acquisition of Intelsat...\n",
      "2025-06-13 12:11:50,442 - INFO - Processing article 10/30: Sensorless FOC for stepper motor driver...\n",
      "2025-06-13 12:11:50,902 - INFO - Progress: 10/30 - Success: 10, Failed: 0\n",
      "2025-06-13 12:11:51,865 - INFO - Processing article 11/30: Alice & Bob adopt CUDA and run 75x faster...\n",
      "2025-06-13 12:11:53,336 - INFO - Processing article 12/30: Time-to-digital conversion for space applications...\n",
      "2025-06-13 12:12:38,419 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /time-to-digital-conversion-for-space-applications/\n",
      "2025-06-13 12:13:25,523 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /time-to-digital-conversion-for-space-applications/\n",
      "2025-06-13 12:13:47,677 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /time-to-digital-conversion-for-space-applications/\n",
      "2025-06-13 12:16:13,936 - INFO - Processing article 13/30: The FCC Builds a Firewall Around US-Bound Electron...\n",
      "2025-06-13 12:16:58,948 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /the-fcc-builds-a-firewall-around-us-bound-electronics/\n",
      "2025-06-13 12:17:46,138 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /the-fcc-builds-a-firewall-around-us-bound-electronics/\n",
      "2025-06-13 12:18:08,332 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /the-fcc-builds-a-firewall-around-us-bound-electronics/\n",
      "2025-06-13 12:20:37,373 - INFO - Processing article 14/30: GD32C231 Series MCU — Redefining Cost-Performance,...\n",
      "2025-06-13 12:21:22,487 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /gd32c231-series-mcu-redefining-cost-performance-unleashing-new-potential/\n",
      "2025-06-13 12:22:09,565 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /gd32c231-series-mcu-redefining-cost-performance-unleashing-new-potential/\n",
      "2025-06-13 12:22:58,657 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /gd32c231-series-mcu-redefining-cost-performance-unleashing-new-potential/\n",
      "2025-06-13 12:24:55,354 - INFO - Processing article 15/30: Cupola360 RX2000: Smart Patrol Camera at COMPUTEX ...\n",
      "2025-06-13 12:25:40,416 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /cupola360-rx2000-smart-patrol-camera-at-computex-for-real-time-spatial-awareness/\n",
      "2025-06-13 12:26:27,541 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /cupola360-rx2000-smart-patrol-camera-at-computex-for-real-time-spatial-awareness/\n",
      "2025-06-13 12:27:16,610 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /cupola360-rx2000-smart-patrol-camera-at-computex-for-real-time-spatial-awareness/\n",
      "2025-06-13 12:29:39,474 - INFO - Progress: 15/30 - Success: 15, Failed: 0\n",
      "2025-06-13 12:29:43,148 - INFO - Processing article 16/30: Indian Automotive OEMs Stand to Benefit from an Op...\n",
      "2025-06-13 12:30:28,223 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /indian-automotive-oems-stand-to-benefit-from-an-open-gmsl/\n",
      "2025-06-13 12:31:15,296 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /indian-automotive-oems-stand-to-benefit-from-an-open-gmsl/\n",
      "2025-06-13 12:32:04,368 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /indian-automotive-oems-stand-to-benefit-from-an-open-gmsl/\n",
      "2025-06-13 12:34:30,821 - INFO - Processing article 17/30: Altum amps speed RF design with Quantic blocks...\n",
      "2025-06-13 12:35:15,927 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /altum-amps-speed-rf-design-with-quantic-blocks/\n",
      "2025-06-13 12:36:03,172 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /altum-amps-speed-rf-design-with-quantic-blocks/\n",
      "2025-06-13 12:36:25,296 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /altum-amps-speed-rf-design-with-quantic-blocks/\n",
      "2025-06-13 12:38:24,775 - INFO - Processing article 18/30: Amp elevates K-band throughput for LEO sats...\n",
      "2025-06-13 12:38:27,488 - INFO - Processing article 19/30: Simultaneous sweep boosts multi-VNA test speed...\n",
      "2025-06-13 12:38:30,106 - INFO - Processing article 20/30: Eval board eases battery motor-drive design...\n",
      "2025-06-13 12:38:30,271 - INFO - Progress: 20/30 - Success: 20, Failed: 0\n",
      "2025-06-13 12:38:33,677 - INFO - Processing article 21/30: MCUs enable USB-C Rev 2.4 designs...\n",
      "2025-06-13 12:39:18,687 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /mcus-enable-usb-c-rev-2-4-designs/\n",
      "2025-06-13 12:40:05,749 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /mcus-enable-usb-c-rev-2-4-designs/\n",
      "2025-06-13 12:40:54,817 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /mcus-enable-usb-c-rev-2-4-designs/\n",
      "2025-06-13 12:42:24,470 - INFO - Processing article 22/30: The 2025 WWDC: From Intel, Apple’s Nearly Free, an...\n",
      "2025-06-13 12:42:26,732 - INFO - Processing article 23/30: AI Demand Drives Disaggregated Storage...\n",
      "2025-06-13 12:43:11,799 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /ai-demand-drives-disaggregated-storage/\n",
      "2025-06-13 12:43:58,881 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /ai-demand-drives-disaggregated-storage/\n",
      "2025-06-13 12:44:47,953 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /ai-demand-drives-disaggregated-storage/\n",
      "2025-06-13 12:46:46,306 - INFO - Processing article 24/30: Innatera Adds More Accelerators to Spiking Microco...\n",
      "2025-06-13 12:46:49,774 - INFO - Processing article 25/30: Can Neocloud Architectures Break the AI Infra Bott...\n",
      "2025-06-13 12:47:34,783 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /can-neocloud-architectures-break-the-ai-infra-bottleneck/\n",
      "2025-06-13 12:48:21,931 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /can-neocloud-architectures-break-the-ai-infra-bottleneck/\n",
      "2025-06-13 12:49:11,005 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /can-neocloud-architectures-break-the-ai-infra-bottleneck/\n",
      "2025-06-13 12:51:16,202 - INFO - Progress: 25/30 - Success: 25, Failed: 0\n",
      "2025-06-13 12:51:18,766 - INFO - Processing article 26/30: A two-way Wilson current mirror...\n",
      "2025-06-13 12:51:21,380 - INFO - Processing article 27/30: China Sharpens Strategy in the Global 6G Race...\n",
      "2025-06-13 12:52:06,475 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /china-sharpens-strategy-in-the-global-6g-race/\n",
      "2025-06-13 12:52:53,558 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /china-sharpens-strategy-in-the-global-6g-race/\n",
      "2025-06-13 12:53:42,634 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /china-sharpens-strategy-in-the-global-6g-race/\n",
      "2025-06-13 12:55:39,932 - INFO - Processing article 28/30: MEMS Speakers Enable Full-Range Audio in Wearables...\n",
      "2025-06-13 12:55:43,489 - INFO - Processing article 29/30: A two-wire temperature transmitter using an RTD se...\n",
      "2025-06-13 12:55:47,144 - INFO - Processing article 30/30: Tony Pialis’ design journey to high-speed SerDes...\n",
      "2025-06-13 12:55:47,321 - INFO - Progress: 30/30 - Success: 30, Failed: 0\n",
      "2025-06-13 12:55:51,862 - INFO - Total articles processed: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully\n"
     ]
    }
   ],
   "source": [
    "collected_data = collect_article_data('./intermediate_data/01-Collect/Scraped_Products_Article_Links.csv')\n",
    "with open('./intermediate_data/01-Collect/Scraped_Products_Article_Data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(collected_data, f, indent=2, ensure_ascii=False)\n",
    "print(\"Data saved successfully\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
