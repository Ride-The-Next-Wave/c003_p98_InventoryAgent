{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Article & Product Matching Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file consists of a Gensim Model that inputs scraped article data and DigiKey product list to perform a semantic and keyword matching analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open(\"./intermediate_data/Cleaned_Article_Data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "with open(\"./intermediate_data/Products_List_Clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "products = raw_data.get(\"categories\", [])\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "article_texts = []\n",
    "for article in articles:\n",
    "    text = article.get(\"text\")\n",
    "    if text: \n",
    "        article_texts.append(preprocess(text))\n",
    "product_texts = [preprocess(f\"{p['name']} {p.get('category', '')}\") for p in products]\n",
    "all_texts = article_texts + product_texts\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = Dictionary(all_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in all_texts]\n",
    "\n",
    "# Build TF-IDF model\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# Split article and product vectors\n",
    "article_tfidf_vectors = corpus_tfidf[:len(articles)]\n",
    "product_tfidf_vectors = corpus_tfidf[len(articles):]\n",
    "\n",
    "# Articles Similarity Index\n",
    "article_index = SparseMatrixSimilarity(article_tfidf_vectors, num_features=len(dictionary))\n",
    "\n",
    "# 1. Keyword Matching\n",
    "def find_articles_mentioning_product(product_name):\n",
    "    mentions = []\n",
    "    product_name_lower = product_name.lower()\n",
    "    for i, article in enumerate(articles):\n",
    "        text = article.get('text')\n",
    "        if text and product_name_lower in text.lower():\n",
    "            mentions.append((i, article))\n",
    "    return mentions\n",
    "\n",
    "# 2. Semantic Matching \n",
    "def recommend_similar_articles(product_name, product_category, top_n=5):\n",
    "    product_text = f\"{product_name} {product_category}\"\n",
    "    bow = dictionary.doc2bow(preprocess(product_text))\n",
    "    tfidf_vec = tfidf[bow]\n",
    "    sims = article_index[tfidf_vec]\n",
    "    ranked = sorted(enumerate(sims), key=lambda x: x[1], reverse=True)\n",
    "    seen_articles = {} # Removing duplicates\n",
    "    unique_results = []\n",
    "\n",
    "    for i, score in ranked:\n",
    "        article = articles[i]\n",
    "        article_key = article['title']\n",
    "        if article_key not in seen_articles:\n",
    "            seen_articles[article_key] = True\n",
    "            unique_results.append((article, (score*100)))\n",
    "        if len(unique_results) >= top_n:\n",
    "            break\n",
    "    return unique_results\n",
    "\n",
    "# Select random products\n",
    "noOfProducts=70\n",
    "random.seed(noOfProducts)\n",
    "random_products = random.sample(products, min(noOfProducts, len(products)))\n",
    "all_results = []\n",
    "\n",
    "# Number of top articles to include per product\n",
    "top_n = 3\n",
    "final_rows = []\n",
    "for product in random_products:\n",
    "    results = recommend_similar_articles(product['name'], product.get('category', ''), top_n=top_n) # Article recommendations\n",
    "    valid_results = [(article, score) for article, score in results if score > 0]  # Filter results with score > 0\n",
    "    if not valid_results:\n",
    "       continue\n",
    "    row = {\n",
    "        'Product Category': product['name'],\n",
    "        'Items': product.get('Products', 0),\n",
    "        'Product url': product.get('url', '')\n",
    "    }\n",
    "    article_idx = 1\n",
    "    for article, score in results:\n",
    "        if score > 0:\n",
    "            row[f'Article_{article_idx}_Score'] = round(score, 3)\n",
    "            row[f'Article_{article_idx}_Title'] = article.get('title', '')\n",
    "            row[f'Article_{article_idx}_Link'] = article.get('link', '') \n",
    "            article_idx += 1\n",
    "    final_rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Matching Results\n",
    "\n",
    "Saves the result of the model to Product_Article_Matching file in intermediate data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_wide = pd.DataFrame(final_rows)\n",
    "df_wide.to_csv('./intermediate_data/Product_Article_Matching.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
