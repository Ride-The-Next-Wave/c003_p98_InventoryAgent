{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we collect recent articles by scraping different sites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDN Network, https://www.edn.com/?s=Practical+design+articles+and+component+selection+guides\n",
    "\n",
    "EE Times, https://www.eetimes.com/?s=New+component+announcements%2C+technology+trends%2C+and+design+techniques\n",
    "\n",
    "Electronic Design, https://www.electronicdesign.com/search?page=1&filters=%7B%22text%22%3A%22Design%20methodologies%20and%20component%20comparisons%22%2C%22page%22%3A1%2C%22status%22%3A%5B%221%22%5D%2C%22impliedSchedules%22%3Atrue%7D&sort=score\n",
    "\n",
    "Electronics Weekly, https://www.electronicsweekly.com/?s=Product+launches+and+industry+news&type=&category=0&year=0&orderby=name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping EDN Network...\n",
      "  Attempting to fetch: https://www.edn.com/?s=Practical+design+articles+and+component+selection+guides (attempt 1)\n",
      "  Request error on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "  Attempting to fetch: https://www.edn.com/?s=Practical+design+articles+and+component+selection+guides (attempt 2)\n",
      "  Timeout on attempt 2\n",
      "  Attempting to fetch: https://www.edn.com/?s=Practical+design+articles+and+component+selection+guides (attempt 3)\n",
      "  Timeout on attempt 3\n",
      "Found 0 articles from May 2025 from EDN Network\n",
      "Scraping EE Times...\n",
      "  Attempting to fetch: https://www.eetimes.com/?s=New+component+announcements%2C+technology+trends%2C+and+design+techniques (attempt 1)\n",
      "  Request error on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "  Attempting to fetch: https://www.eetimes.com/?s=New+component+announcements%2C+technology+trends%2C+and+design+techniques (attempt 2)\n",
      "  Timeout on attempt 2\n",
      "  Attempting to fetch: https://www.eetimes.com/?s=New+component+announcements%2C+technology+trends%2C+and+design+techniques (attempt 3)\n",
      "  Timeout on attempt 3\n",
      "Found 0 articles from May 2025 from EE Times\n",
      "Scraping Electronic Design...\n",
      "  Attempting to fetch: https://www.electronicdesign.com/search?page=1&filters=%7B%22text%22%3A%22Design%20methodologies%20and%20component%20comparisons%22%2C%22page%22%3A1%2C%22status%22%3A%5B%221%22%5D%2C%22impliedSchedules%22%3Atrue%7D&sort=score (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Page title: No title found\n",
      "  Fallback: Found 0 links\n",
      "Found 0 articles from May 2025 from Electronic Design\n",
      "Scraping Electronics Weekly...\n",
      "  Attempting to fetch: https://www.electronicsweekly.com/?s=Product+launches+and+industry+news&type=&category=0&year=0&orderby=name (attempt 1)\n",
      "  Page title: Search Results for :: Electronics Weekly\n",
      "  Found 10 elements with selector: article\n",
      "    Added: Elektra Awards 2025 open for entries...\n",
      "    Added: NMI hosts industry conference in Glasgow with them...\n",
      "    Added: DigiKey introduces own-brand DigiKey Standard prod...\n",
      "    Added: CHIIPS podcast interview with industry veteran Ash...\n",
      "    Added: Get Mannerisms, Gadget Master, the Daily and the W...\n",
      "    Added: Elektra Awards 2025 looking for tech stars â€“ compa...\n",
      "    Added: IPC praises President Trump for defence industry s...\n",
      "    Added: IPC sets the industry on the path to sustainabilit...\n",
      "    Added: Imec launches Stuttgart advanced chip design accel...\n",
      "    Added: Auto hi-voltage detector claims fastest response t...\n",
      "Found 10 articles from May 2025 from Electronics Weekly\n",
      "\n",
      "=== SCRAPING RESULTS ===\n",
      "Total articles found: 10\n",
      "Saved 10 articles to ./intermediate_data/scraped_articles.csv\n",
      "Articles by source:\n",
      "  Electronics Weekly: 10 articles\n"
     ]
    }
   ],
   "source": [
    "class ArticleDataCollector:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                print(f\"Fetching: {url} (Attempt {attempt + 1})\")\n",
    "                response = self.session.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                time.sleep(5)\n",
    "        return None\n",
    "\n",
    "    def extract_article_data(self, element, base_url):\n",
    "        try:\n",
    "            title = (element.find(['h1', 'h2', 'h3', 'h4', 'h5']) or\n",
    "                     element.find(class_=re.compile(\"title|headline\", re.I)) or\n",
    "                     element.get('title') or\n",
    "                     element.get('aria-label') or\n",
    "                     element.get_text(strip=True))[:100]\n",
    "\n",
    "            link_tag = element if element.name == 'a' and element.get('href') else element.find('a', href=True)\n",
    "            link = urljoin(base_url, link_tag['href']) if link_tag else ''\n",
    "\n",
    "            date_elem = element.find(['time', 'span', 'div'], \n",
    "                class_=re.compile(r'date|time|publish|created|updated', re.I)) or \\\n",
    "                element.find(attrs={'datetime': True}) or \\\n",
    "                element.find(attrs={'data-date': True})\n",
    "            date_str = date_elem.get('datetime') or date_elem.get('data-date') or date_elem.get_text(strip=True) if date_elem else ''\n",
    "\n",
    "            return title.strip(), link.strip(), date_str.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"  Extraction error: {e}\")\n",
    "            return \"\", \"\", \"\"\n",
    "\n",
    "    def scrape_website(self, url, source):\n",
    "        articles, seen_links = [], set()\n",
    "        response = self.make_request(url)\n",
    "        if not response:\n",
    "            return articles\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(f\"  Title: {soup.title.string if soup.title else 'No title'}\")\n",
    "\n",
    "        selectors = [\n",
    "            'article', '.post', '.story', '.entry', '.search-result',\n",
    "            '.article-item', '.news-item', '.content-item',\n",
    "            '[class*=\"post\"]', '[class*=\"article\"]', '[class*=\"story\"]',\n",
    "            '[class*=\"result\"]', '[class*=\"item\"]', '[class*=\"news\"]'\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            if elements:\n",
    "                print(f\"  Found {len(elements)} articles with selector: {selector}\")\n",
    "                break\n",
    "        else:\n",
    "            elements = soup.find_all(['main', 'section', 'div'], class_=re.compile('content|articles|posts|results', re.I))\n",
    "            elements = elements[0].find_all('a', href=True) if elements else soup.find_all('a', href=True)\n",
    "            print(f\"  Fallback: Found {len(elements)} links\")\n",
    "\n",
    "        for element in elements[:50]:\n",
    "            title, link, date_str = self.extract_article_data(element, url)\n",
    "            if title and link and len(title) > 10 and link not in seen_links:\n",
    "                if not any(skip in link.lower() for skip in ['javascript:', 'mailto:', '#', 'tel:']):\n",
    "                    seen_links.add(link)\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'link': link,\n",
    "                        'date_raw': date_str or \"Date not found\",\n",
    "                        'source': source,\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    })\n",
    "                    print(f\"    Collected: {title[:50]}\")\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def collect_from_sources(self, csv_path):\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"CSV not found: {csv_path}\")\n",
    "            return []\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} sources\")\n",
    "\n",
    "        all_articles = []\n",
    "        for _, row in df.iterrows():\n",
    "            source, url = str(row.get('Article', '')).strip(), str(row.get('Link', '')).strip()\n",
    "            if not url or url.lower() == 'nan':\n",
    "                continue\n",
    "            print(f\"\\nScraping {source}...\")\n",
    "            all_articles.extend(self.scrape_website(url, source))\n",
    "            time.sleep(3)  # Respectful delay\n",
    "        return all_articles\n",
    "\n",
    "    def get_summary(self, articles):\n",
    "        if not articles:\n",
    "            return \"No articles collected.\"\n",
    "\n",
    "        summary = {\n",
    "            'total_articles': len(articles),\n",
    "            'by_source': {},\n",
    "            'with_dates': sum(1 for a in articles if a['date_raw'] != \"Date not found\"),\n",
    "            'without_dates': sum(1 for a in articles if a['date_raw'] == \"Date not found\")\n",
    "        }\n",
    "\n",
    "        for a in articles:\n",
    "            summary['by_source'][a['source']] = summary['by_source'].get(a['source'], 0) + 1\n",
    "\n",
    "        return summary\n",
    "    \n",
    "def main():\n",
    "    scraper = ArticleScraper()\n",
    "    \n",
    "    # Scrape articles from all sources\n",
    "    articles = scraper.scrape_all_sources('./data/article-links.csv')\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n=== SCRAPING RESULTS ===\")\n",
    "    print(f\"Total articles found: {len(articles)}\")\n",
    "    \n",
    "    if articles:\n",
    "        \n",
    "        # Save to CSV\n",
    "        scraper.save_results(articles)\n",
    "        \n",
    "        # Group by source for summary\n",
    "        by_source = {}\n",
    "        for article in articles:\n",
    "            source = article['source']\n",
    "            if source not in by_source:\n",
    "                by_source[source] = 0\n",
    "            by_source[source] += 1\n",
    "        \n",
    "        print(\"Articles by source:\")\n",
    "        for source, count in by_source.items():\n",
    "            print(f\"  {source}: {count} articles\")\n",
    "    else:\n",
    "        print(\"No recent articles found. This might be due to:\")\n",
    "        print(\"- Website structure changes\")\n",
    "        print(\"- Anti-scraping measures\")\n",
    "        print(\"- Network issues\")\n",
    "        print(\"- No articles from May 2025 specifically\")\n",
    "        print(\"- Date parsing difficulties\")\n",
    "        print(\"\\nTip: Check if the websites have articles from May 2025, as we're specifically filtering for that month.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation_libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
