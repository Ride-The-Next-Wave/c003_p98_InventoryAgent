{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we collect recent articles by scraping different sites such as EDN Network, EE Times, Electronic Design, Electronics Weekly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDN Network, https://www.edn.com/?s=Practical+design+articles+and+component+selection+guides\n",
    "\n",
    "EE Times, https://www.eetimes.com/?s=New+component+announcements%2C+technology+trends%2C+and+design+techniques\n",
    "\n",
    "Electronic Design, https://www.electronicdesign.com/search?page=1&filters=%7B%22text%22%3A%22Design%20methodologies%20and%20component%20comparisons%22%2C%22page%22%3A1%2C%22status%22%3A%5B%221%22%5D%2C%22impliedSchedules%22%3Atrue%7D&sort=score\n",
    "\n",
    "Electronics Weekly, https://www.electronicsweekly.com/?s=Product+launches+and+industry+news&type=&category=0&year=0&orderby=name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 articles from May 2025 from EDN Network\n",
      "Found 0 articles from May 2025 from EE Times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Page title: No title found\n",
      "  Fallback: Found 0 links\n",
      "Found 0 articles from May 2025 from Electronic Design\n",
      "  Page title: Search Results for :: Electronics Weekly\n",
      "  Found 10 elements with selector: article\n",
      "    Added: Elektra Awards 2025 open for entries...\n",
      "    Added: NMI hosts industry conference in Glasgow with them...\n",
      "    Added: DigiKey introduces own-brand DigiKey Standard prod...\n",
      "    Added: CHIIPS podcast interview with industry veteran Ash...\n",
      "    Added: Get Mannerisms, Gadget Master, the Daily and the W...\n",
      "    Added: Elektra Awards 2025 looking for tech stars â€“ compa...\n",
      "    Added: IPC praises President Trump for defence industry s...\n",
      "    Added: IPC sets the industry on the path to sustainabilit...\n",
      "    Added: Imec launches Stuttgart advanced chip design accel...\n",
      "    Added: Auto hi-voltage detector claims fastest response t...\n",
      "Found 10 articles from May 2025 from Electronics Weekly\n",
      "\n",
      "--- SCRAPING RESULTS ---\n",
      "Total articles found: 10\n",
      "Saved 10 articles to ./intermediate_data/Scraped_Article_Links.csv\n",
      "Articles by source:\n",
      "  Electronics Weekly: 10 articles\n"
     ]
    }
   ],
   "source": [
    "class ArticleScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0'\n",
    "        })\n",
    "        self.target_month = 5 \n",
    "        self.target_year = 2025\n",
    "        \n",
    "    def parse_date(self, date_str):\n",
    "        \"\"\"Parse various date formats commonly found on websites\"\"\"\n",
    "        if not date_str:\n",
    "            return None\n",
    "        # Clean the date strings\n",
    "        date_str = date_str.strip().lower()\n",
    "        patterns = [\n",
    "            r'(\\d{1,2})/(\\d{1,2})/(\\d{4})',  # MM/DD/YYYY\n",
    "            r'(\\d{4})-(\\d{1,2})-(\\d{1,2})',  # YYYY-MM-DD\n",
    "            r'(\\d{1,2})-(\\d{1,2})-(\\d{4})',  # DD-MM-YYYY\n",
    "            r'(\\w+)\\s+(\\d{1,2}),?\\s+(\\d{4})',  # Month DD, YYYY\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, date_str)\n",
    "            if match:\n",
    "                try:\n",
    "                    if 'month' in pattern:\n",
    "                        month_name, day, year = match.groups()\n",
    "                        month_dict = {\n",
    "                            'january': 1, 'february': 2, 'march': 3, 'april': 4,\n",
    "                            'may': 5, 'june': 6, 'july': 7, 'august': 8,\n",
    "                            'september': 9, 'october': 10, 'november': 11, 'december': 12,\n",
    "                            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "                            'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "                        }\n",
    "                        month = month_dict.get(month_name.lower())\n",
    "                        if month:\n",
    "                            return datetime(int(year), month, int(day))\n",
    "                    else:\n",
    "                        parts = match.groups()\n",
    "                        if len(parts) == 3:\n",
    "                            if len(parts[0]) == 4:  # YYYY-MM-DD\n",
    "                                return datetime(int(parts[0]), int(parts[1]), int(parts[2]))\n",
    "                            else:  # MM/DD/YYYY or DD-MM-YYYY\n",
    "                                return datetime(int(parts[2]), int(parts[0]), int(parts[1]))\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def is_recent(self, date_obj):\n",
    "        if not date_obj:\n",
    "            return True  # Include articles without clear dates for manual review\n",
    "        return date_obj.year == self.target_year and date_obj.month == self.target_month\n",
    "    \n",
    "    def make_request(self, url, max_retries=3):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=30)  # Increased timeout\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.exceptions.Timeout:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)  # Wait before retry\n",
    "                continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5)\n",
    "                continue\n",
    "        return None\n",
    "    \n",
    "    def scrape_edn_network(self, url):\n",
    "        articles = []\n",
    "        try:\n",
    "            response = self.make_request(url)\n",
    "            if not response:\n",
    "                return articles\n",
    "                \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print(f\"  Page title: {soup.title.string if soup.title else 'No title found'}\")\n",
    "            selectors = [\n",
    "                'article',\n",
    "                '.post',\n",
    "                '.entry',\n",
    "                '.search-result',\n",
    "                '.article-item',\n",
    "                '[class*=\"post\"]',\n",
    "                '[class*=\"article\"]'\n",
    "            ]\n",
    "            article_elements = []\n",
    "            for selector in selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    article_elements = elements\n",
    "                    print(f\"  Found {len(elements)} elements with selector: {selector}\")\n",
    "                    break\n",
    "            \n",
    "            if not article_elements:\n",
    "                article_elements = soup.find_all('a', href=True)\n",
    "                print(f\"  Fallback: Found {len(article_elements)} links\")\n",
    "            for element in article_elements[:30]:  # Increased limit\n",
    "                title = \"\"\n",
    "                link = \"\"\n",
    "                date_str = \"\"\n",
    "                if hasattr(element, 'get_text'):\n",
    "                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "                    if title_elem:\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "                    elif element.name == 'a':\n",
    "                        title = element.get_text(strip=True)\n",
    "                    else:\n",
    "                        title = element.get_text(strip=True)[:100]\n",
    "                if element.name == 'a' and element.get('href'):\n",
    "                    link = urljoin(url, element['href'])\n",
    "                else:\n",
    "                    link_elem = element.find('a', href=True)\n",
    "                    if link_elem:\n",
    "                        link = urljoin(url, link_elem['href'])\n",
    "                date_elem = element.find(['time', 'span', 'div'], class_=re.compile(r'date|time|publish', re.I))\n",
    "                if date_elem:\n",
    "                    date_str = date_elem.get_text(strip=True)\n",
    "                if title and link and len(title) > 10:  \n",
    "                    date_obj = self.parse_date(date_str)\n",
    "                    \n",
    "                    if self.is_recent(date_obj):\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'date': date_str or \"Date not found\",\n",
    "                            'source': 'EDN Network'\n",
    "                        })\n",
    "                        print(f\"    Added: {title[:50]}...\")  \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping EDN Network: {e}\")\n",
    "        return articles\n",
    "    \n",
    "    def scrape_ee_times(self, url):\n",
    "        \"\"\"Scrape EE Times articles\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            response = self.make_request(url)\n",
    "            if not response:\n",
    "                return articles\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print(f\"  Page title: {soup.title.string if soup.title else 'No title found'}\")\n",
    "            \n",
    "            # Multiple selectors to try\n",
    "            selectors = [\n",
    "                'article',\n",
    "                '.story',\n",
    "                '.post',\n",
    "                '.search-result',\n",
    "                '.article-item',\n",
    "                '[class*=\"story\"]',\n",
    "                '[class*=\"post\"]'\n",
    "            ]\n",
    "            \n",
    "            article_elements = []\n",
    "            for selector in selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    article_elements = elements\n",
    "                    print(f\"  Found {len(elements)} elements with selector: {selector}\")\n",
    "                    break\n",
    "            \n",
    "            if not article_elements:\n",
    "                article_elements = soup.find_all('a', href=True)\n",
    "                print(f\"  Fallback: Found {len(article_elements)} links\")\n",
    "            \n",
    "            for element in article_elements[:30]:\n",
    "                title = \"\"\n",
    "                link = \"\"\n",
    "                date_str = \"\"\n",
    "                \n",
    "                # Extract title\n",
    "                if hasattr(element, 'get_text'):\n",
    "                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "                    if title_elem:\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "                    elif element.name == 'a':\n",
    "                        title = element.get_text(strip=True)\n",
    "                    else:\n",
    "                        title = element.get_text(strip=True)[:100]\n",
    "                \n",
    "                # Extract link\n",
    "                if element.name == 'a' and element.get('href'):\n",
    "                    link = urljoin(url, element['href'])\n",
    "                else:\n",
    "                    link_elem = element.find('a', href=True)\n",
    "                    if link_elem:\n",
    "                        link = urljoin(url, link_elem['href'])\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = element.find(['time', 'span', 'div'], class_=re.compile(r'date|time|publish', re.I))\n",
    "                if date_elem:\n",
    "                    date_str = date_elem.get_text(strip=True)\n",
    "                if title and link and len(title) > 10:\n",
    "                    date_obj = self.parse_date(date_str)\n",
    "                    if self.is_recent(date_obj):\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'date': date_str or \"Date not found\",\n",
    "                            'source': 'EE Times'\n",
    "                        })\n",
    "                        print(f\"    Added: {title[:50]}...\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping EE Times: {e}\")  \n",
    "        return articles\n",
    "    \n",
    "    def scrape_electronic_design(self, url):\n",
    "        \"\"\"Scrape Electronic Design articles\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            response = self.make_request(url)\n",
    "            if not response:\n",
    "                return articles   \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print(f\"  Page title: {soup.title.string if soup.title else 'No title found'}\")\n",
    "            # Multiple selectors to try\n",
    "            selectors = [\n",
    "                '.search-result',\n",
    "                '.result-item',\n",
    "                'article',\n",
    "                '.post',\n",
    "                '.item',\n",
    "                '[class*=\"result\"]',\n",
    "                '[class*=\"item\"]'\n",
    "            ]\n",
    "            \n",
    "            article_elements = []\n",
    "            for selector in selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    article_elements = elements\n",
    "                    print(f\"  Found {len(elements)} elements with selector: {selector}\")\n",
    "                    break\n",
    "            \n",
    "            if not article_elements:\n",
    "                article_elements = soup.find_all('a', href=True)\n",
    "                print(f\"  Fallback: Found {len(article_elements)} links\")\n",
    "            for element in article_elements[:30]:\n",
    "                title = \"\"\n",
    "                link = \"\"\n",
    "                date_str = \"\"\n",
    "                if hasattr(element, 'get_text'):\n",
    "                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "                    if title_elem:\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "                    elif element.name == 'a':\n",
    "                        title = element.get_text(strip=True)\n",
    "                    else:\n",
    "                        title = element.get_text(strip=True)[:100]\n",
    "                \n",
    "                # Extract link\n",
    "                if element.name == 'a' and element.get('href'):\n",
    "                    link = urljoin(url, element['href'])\n",
    "                else:\n",
    "                    link_elem = element.find('a', href=True)\n",
    "                    if link_elem:\n",
    "                        link = urljoin(url, link_elem['href'])\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = element.find(['time', 'span', 'div'], class_=re.compile(r'date|time|publish', re.I))\n",
    "                if date_elem:\n",
    "                    date_str = date_elem.get_text(strip=True)\n",
    "                \n",
    "                if title and link and len(title) > 10:\n",
    "                    date_obj = self.parse_date(date_str)\n",
    "                    \n",
    "                    if self.is_recent(date_obj):\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'date': date_str or \"Date not found\",\n",
    "                            'source': 'Electronic Design'\n",
    "                        })\n",
    "                        print(f\"    Added: {title[:50]}...\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Electronic Design: {e}\") \n",
    "        return articles\n",
    "    \n",
    "    def scrape_electronics_weekly(self, url):\n",
    "        \"\"\"Scrape Electronics Weekly articles\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            response = self.make_request(url)\n",
    "            if not response:\n",
    "                return articles\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print(f\"  Page title: {soup.title.string if soup.title else 'No title found'}\")\n",
    "            \n",
    "            # Multiple selectors to try\n",
    "            selectors = [\n",
    "                'article',\n",
    "                '.post',\n",
    "                '.story',\n",
    "                '.search-result',\n",
    "                '.article-item',\n",
    "                '[class*=\"post\"]',\n",
    "                '[class*=\"story\"]'\n",
    "            ]\n",
    "            \n",
    "            article_elements = []\n",
    "            for selector in selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    article_elements = elements\n",
    "                    print(f\"  Found {len(elements)} elements with selector: {selector}\")\n",
    "                    break\n",
    "            \n",
    "            if not article_elements:\n",
    "                article_elements = soup.find_all('a', href=True)\n",
    "                print(f\"  Fallback: Found {len(article_elements)} links\")\n",
    "            \n",
    "            for element in article_elements[:30]:\n",
    "                title = \"\"\n",
    "                link = \"\"\n",
    "                date_str = \"\"\n",
    "                if hasattr(element, 'get_text'):\n",
    "                    title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "                    if title_elem:\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "                    elif element.name == 'a':\n",
    "                        title = element.get_text(strip=True)\n",
    "                    else:\n",
    "                        title = element.get_text(strip=True)[:100]\n",
    "                \n",
    "                # Extract link\n",
    "                if element.name == 'a' and element.get('href'):\n",
    "                    link = urljoin(url, element['href'])\n",
    "                else:\n",
    "                    link_elem = element.find('a', href=True)\n",
    "                    if link_elem:\n",
    "                        link = urljoin(url, link_elem['href'])\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = element.find(['time', 'span', 'div'], class_=re.compile(r'date|time|publish', re.I))\n",
    "                if date_elem:\n",
    "                    date_str = date_elem.get_text(strip=True)\n",
    "                \n",
    "                if title and link and len(title) > 10:\n",
    "                    date_obj = self.parse_date(date_str)\n",
    "                    \n",
    "                    if self.is_recent(date_obj):\n",
    "                        articles.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'date': date_str or \"Date not found\",\n",
    "                            'source': 'Electronics Weekly'\n",
    "                        })\n",
    "                        print(f\"    Added: {title[:50]}...\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Electronics Weekly: {e}\")\n",
    "        return articles\n",
    "    \n",
    "    def scrape_all_sources(self, csv_file_path):\n",
    "        \"\"\"Main function to scrape all sources from CSV\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                source_name = row['Article'].strip()\n",
    "                url = row['Link'].strip()\n",
    "                if 'EDN Network' in source_name:\n",
    "                    articles = self.scrape_edn_network(url)\n",
    "                elif 'EE Times' in source_name:\n",
    "                    articles = self.scrape_ee_times(url)\n",
    "                elif 'Electronic Design' in source_name:\n",
    "                    articles = self.scrape_electronic_design(url)\n",
    "                elif 'Electronics Weekly' in source_name:\n",
    "                    articles = self.scrape_electronics_weekly(url)\n",
    "                else:\n",
    "                    print(f\"Unknown source: {source_name}\")\n",
    "                    continue\n",
    "                \n",
    "                all_articles.extend(articles)\n",
    "                print(f\"Found {len(articles)} articles from May 2025 from {source_name}\")\n",
    "                \n",
    "                # Be respectful - add delay between requests\n",
    "                time.sleep(3)  # Increased delay\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file: {e}\")\n",
    "            return []\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def save_results(self, articles, output_file='./intermediate_data/Scraped_Article_Links.csv'):\n",
    "        if not articles:\n",
    "            print(\"No articles found to save.\")\n",
    "            return\n",
    "        df = pd.DataFrame(articles)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(articles)} articles to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    scraper = ArticleScraper()\n",
    "    articles = scraper.scrape_all_sources('./data/Article_Links.csv')\n",
    "    print(f\"\\n--- SCRAPING RESULTS ---\")\n",
    "    print(f\"Total articles found: {len(articles)}\")\n",
    "    if articles:\n",
    "        scraper.save_results(articles)\n",
    "        by_source = {}\n",
    "        for article in articles:\n",
    "            source = article['source']\n",
    "            if source not in by_source:\n",
    "                by_source[source] = 0\n",
    "            by_source[source] += 1\n",
    "        print(\"Articles by source:\")\n",
    "        for source, count in by_source.items():\n",
    "            print(f\"  {source}: {count} articles\")\n",
    "    else:\n",
    "        print(\"No recent articles found. This might be due to:\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
